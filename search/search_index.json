{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"body { background-image: url('https://miro.medium.com/max/1838/1*HpKh-MGZIq-GyRAiqEUnSw.png'); background-repeat: no-repeat; background-position: right; background-size: 60%; background-attachment: fixed } CS224n Some of the important concepts, formulas, derivations, assignments, and models encountered during my self-study of CS224n: Natural Language Processing with Deep Learning will be recorded here. Layouts Home Chapters: Word Embeddings: Word2vec GloVe Language Models: Language Models RNN LSTM Summary Neural Machine Translation: Seq2Seq Attention Transformers Pretraining Math: Matrix Derivative SVD Assignments: Assignment 1 Assignment 2 Assignment 3 Assignment 4 Assignment 5 References: Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft) Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. Dive into Deep Learning CS224n: Natural Language Processing with Deep Learning","title":"Home"},{"location":"#cs224n","text":"Some of the important concepts, formulas, derivations, assignments, and models encountered during my self-study of CS224n: Natural Language Processing with Deep Learning will be recorded here.","title":"CS224n"},{"location":"#layouts","text":"Home Chapters: Word Embeddings: Word2vec GloVe Language Models: Language Models RNN LSTM Summary Neural Machine Translation: Seq2Seq Attention Transformers Pretraining Math: Matrix Derivative SVD Assignments: Assignment 1 Assignment 2 Assignment 3 Assignment 4 Assignment 5","title":"Layouts"},{"location":"#references","text":"Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft) Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. Dive into Deep Learning CS224n: Natural Language Processing with Deep Learning","title":"References:"},{"location":"assignments/a2/a2/","text":"Assignment 2 Documentation: CS 224n Assignment #2: word2vec 1 Written: Understanding word2vec (a) The true empirical distribution \\(\\mathbf{y}\\) is a one-hot vector with a 1 for the true outside word o, and the \\(k^{th}\\) entry in \\(\\mathbf{\\hat{y}}\\) indicates the conditional probability of the \\(k^{th}\\) word being an \u2018outside word\u2019 for the given c. So only the term associated with word o remains. \\[ -\\sum_{w \\in V o c a b} y_{w} \\log \\left(\\hat{y}_{w}\\right)=-\\log \\left(\\hat{y}_{o}\\right) \\] (b) \\[ \\begin{aligned} J_{\\text {naive-softmax} }\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right) &=-\\log P(O=o | C=c) \\\\ &= -\\log \\frac{\\exp \\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)} {\\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right)} \\\\ &= - {u}_{o}^{\\top}{v}_{c} + \\log \\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right) \\end{aligned} \\] \\[ \\begin{aligned} \\frac{\\partial J_{\\text {naive-softmax} }\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial v_c} &= -u_o + \\sum_{o \\in \\operatorname{Vocab} }\\frac{\\exp(u_o^\\top v_c)}{\\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial (u_o^\\top v_c)}{\\partial v_c}\\\\ &=-u_o + \\sum_{o \\in \\operatorname{Vocab} } P(O=o | C=c) u_o \\\\ &=- U y + U \\hat y \\\\ &= U(\\hat y - y) \\end{aligned} \\] (c) \\[ \\begin{aligned} \\frac{\\partial J_{\\text {naive-softmax} }\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial u_w} &= -v_c \\mathbb{1}_{\\{w=o\\}} + \\frac{\\exp(u_w^\\top v_c)}{\\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial (u_w^\\top v_c)}{\\partial u_w}\\\\ &=-v_c \\mathbb{1}_{\\{w=o\\}} + P(w | c) v_c \\\\ &=v_c( \\hat y_w - \\mathbb{1}_{\\{w=o\\}}) \\end{aligned} \\] where \\(\\mathbb{1}\\) is the indicator function. (d) \\[ \\frac{\\partial}{\\partial U} \\mathbf{J}_{\\text {naive-softmax }}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)=\\left[\\frac{\\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial \\mathbf{u}_{1}}, \\frac{\\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial \\mathbf{u}_{2}}, \\ldots, \\frac{\\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial \\mathbf{u}_{|V o c a b|}}\\right] \\] Each term can be obtained from Answer (c) . (e) The derivation of ReLU activation funciton is: $$ f(x)= \\begin{cases} 0 & \\text { if } x<0 \\ 1 & \\text { if } x>0\\end{cases} $$ (f) The derivation of sigmoid function is: $$ \\sigma'(x) = \\sigma(x) \\left(1-\\sigma(x)\\right) $$ (g) \\[ \\begin{aligned} &\\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\mathbf{J}_{\\text {neg-sample }}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)=-\\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\log \\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\right)-\\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\sum_{k=1}^{K} \\log \\left(\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)\\\\ &=-\\frac{1}{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-\\sum_{k=1}^{K} \\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\log \\left(\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)\\\\ &=-\\frac{1}{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)} \\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\left(1-\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\right) \\mathbf{u}_{o}-\\sum_{k=1}^{K} \\frac{1}{\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\\\ &=\\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-1\\right) \\mathbf{u}_{o}-\\sum_{k=1}^{K} \\frac{1}{\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)} \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\left(1-\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)\\left(-\\mathbf{u}_{k}\\right)\\\\ &=\\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-1\\right) \\mathbf{u}_{o}+\\sum_{k=1}^{K}\\left(1-\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right) \\mathbf{u}_{k} \\end{aligned} \\] \\[ \\begin{aligned} \\frac{\\partial J_{\\text {neg-sample} }\\left(v_{c}, o, U\\right)}{\\partial u_o} &=-\\frac{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\left(1- \\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\right)}{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)}v _c \\\\ &= \\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-1\\right)v_c \\end{aligned} \\] \\[ \\begin{aligned} \\frac{\\partial J_{\\text {neg-sample} }\\left(v_{c}, o, U\\right)}{\\partial u_k} &= - \\frac{\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\left(1- \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)} {\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)}(-v_c)\\\\ &= \\left(1- \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)v_c \\end{aligned} \\] (h). The derivation here is intuitive, i.e. simply sum them considering the multiplicity. \\[ \\frac{\\partial J_{\\text {neg-sample} }\\left(v_{c}, o, U\\right)}{\\partial u_k} = \\sum_{x=1}^{K}\\mathbb{1}_{\\lbrace u_x=u_k \\rbrace}\\left(1- \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)v_c \\] (i). \\[ \\begin{aligned} \\partial \\mathbf{J}_{\\text {skip-gram } }\\left(\\mathbf{v}_{c}, w_{t-m}, \\dots w_{t+m}, \\mathbf{U}\\right) / \\partial \\mathbf{U} &=\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, w_{t+j}, \\mathbf{U}\\right) / \\partial \\mathbf{U} \\\\ \\partial \\mathbf{J}_{\\text {skip-gram } }\\left(\\mathbf{v}_{c}, w_{t-m}, \\dots w_{t+m}, \\mathbf{U}\\right) / \\partial \\mathbf{v_c} &=\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, w_{t+j}, \\mathbf{U}\\right) / \\partial \\mathbf{v_c} \\\\ \\partial \\mathbf{J}_{\\text {skip-gram } }\\left(\\mathbf{v}_{c}, w_{t-m}, \\dots w_{t+m}, \\mathbf{U}\\right) / \\partial \\mathbf{v_w} &=\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, w_{t+j}, \\mathbf{U}\\right) / \\partial \\mathbf{v_w} =0\\\\ \\end{aligned} \\] 2 Coding: Implementing word2vec Implementation: Assignment2 Code Training Results:","title":"Assignment 2"},{"location":"assignments/a2/a2/#assignment-2","text":"Documentation: CS 224n Assignment #2: word2vec","title":"Assignment 2"},{"location":"assignments/a2/a2/#1-written-understanding-word2vec","text":"","title":"1 Written: Understanding word2vec"},{"location":"assignments/a2/a2/#a","text":"The true empirical distribution \\(\\mathbf{y}\\) is a one-hot vector with a 1 for the true outside word o, and the \\(k^{th}\\) entry in \\(\\mathbf{\\hat{y}}\\) indicates the conditional probability of the \\(k^{th}\\) word being an \u2018outside word\u2019 for the given c. So only the term associated with word o remains. \\[ -\\sum_{w \\in V o c a b} y_{w} \\log \\left(\\hat{y}_{w}\\right)=-\\log \\left(\\hat{y}_{o}\\right) \\]","title":"(a)"},{"location":"assignments/a2/a2/#b","text":"\\[ \\begin{aligned} J_{\\text {naive-softmax} }\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right) &=-\\log P(O=o | C=c) \\\\ &= -\\log \\frac{\\exp \\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)} {\\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right)} \\\\ &= - {u}_{o}^{\\top}{v}_{c} + \\log \\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right) \\end{aligned} \\] \\[ \\begin{aligned} \\frac{\\partial J_{\\text {naive-softmax} }\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial v_c} &= -u_o + \\sum_{o \\in \\operatorname{Vocab} }\\frac{\\exp(u_o^\\top v_c)}{\\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial (u_o^\\top v_c)}{\\partial v_c}\\\\ &=-u_o + \\sum_{o \\in \\operatorname{Vocab} } P(O=o | C=c) u_o \\\\ &=- U y + U \\hat y \\\\ &= U(\\hat y - y) \\end{aligned} \\]","title":"(b)"},{"location":"assignments/a2/a2/#c","text":"\\[ \\begin{aligned} \\frac{\\partial J_{\\text {naive-softmax} }\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial u_w} &= -v_c \\mathbb{1}_{\\{w=o\\}} + \\frac{\\exp(u_w^\\top v_c)}{\\sum_{w \\in \\operatorname{Vocab} } \\exp \\left(\\mathbf{u}_{\\mathbf{w} }^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial (u_w^\\top v_c)}{\\partial u_w}\\\\ &=-v_c \\mathbb{1}_{\\{w=o\\}} + P(w | c) v_c \\\\ &=v_c( \\hat y_w - \\mathbb{1}_{\\{w=o\\}}) \\end{aligned} \\] where \\(\\mathbb{1}\\) is the indicator function.","title":"(c)"},{"location":"assignments/a2/a2/#d","text":"\\[ \\frac{\\partial}{\\partial U} \\mathbf{J}_{\\text {naive-softmax }}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)=\\left[\\frac{\\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial \\mathbf{u}_{1}}, \\frac{\\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial \\mathbf{u}_{2}}, \\ldots, \\frac{\\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)}{\\partial \\mathbf{u}_{|V o c a b|}}\\right] \\] Each term can be obtained from Answer (c) .","title":"(d)"},{"location":"assignments/a2/a2/#e","text":"The derivation of ReLU activation funciton is: $$ f(x)= \\begin{cases} 0 & \\text { if } x<0 \\ 1 & \\text { if } x>0\\end{cases} $$","title":"(e)"},{"location":"assignments/a2/a2/#f","text":"The derivation of sigmoid function is: $$ \\sigma'(x) = \\sigma(x) \\left(1-\\sigma(x)\\right) $$","title":"(f)"},{"location":"assignments/a2/a2/#g","text":"\\[ \\begin{aligned} &\\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\mathbf{J}_{\\text {neg-sample }}\\left(\\mathbf{v}_{c}, o, \\mathbf{U}\\right)=-\\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\log \\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\right)-\\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\sum_{k=1}^{K} \\log \\left(\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)\\\\ &=-\\frac{1}{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-\\sum_{k=1}^{K} \\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\log \\left(\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)\\\\ &=-\\frac{1}{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)} \\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\left(1-\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\right) \\mathbf{u}_{o}-\\sum_{k=1}^{K} \\frac{1}{\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)} \\frac{\\partial}{\\partial \\mathbf{v}_{c}} \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\\\ &=\\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-1\\right) \\mathbf{u}_{o}-\\sum_{k=1}^{K} \\frac{1}{\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)} \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\left(1-\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)\\left(-\\mathbf{u}_{k}\\right)\\\\ &=\\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-1\\right) \\mathbf{u}_{o}+\\sum_{k=1}^{K}\\left(1-\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right) \\mathbf{u}_{k} \\end{aligned} \\] \\[ \\begin{aligned} \\frac{\\partial J_{\\text {neg-sample} }\\left(v_{c}, o, U\\right)}{\\partial u_o} &=-\\frac{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\left(1- \\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)\\right)}{\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)}v _c \\\\ &= \\left(\\sigma\\left(\\mathbf{u}_{o}^{\\top} \\mathbf{v}_{c}\\right)-1\\right)v_c \\end{aligned} \\] \\[ \\begin{aligned} \\frac{\\partial J_{\\text {neg-sample} }\\left(v_{c}, o, U\\right)}{\\partial u_k} &= - \\frac{\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\left(1- \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)} {\\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)}(-v_c)\\\\ &= \\left(1- \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)v_c \\end{aligned} \\]","title":"(g)"},{"location":"assignments/a2/a2/#h","text":"The derivation here is intuitive, i.e. simply sum them considering the multiplicity. \\[ \\frac{\\partial J_{\\text {neg-sample} }\\left(v_{c}, o, U\\right)}{\\partial u_k} = \\sum_{x=1}^{K}\\mathbb{1}_{\\lbrace u_x=u_k \\rbrace}\\left(1- \\sigma\\left(-\\mathbf{u}_{k}^{\\top} \\mathbf{v}_{c}\\right)\\right)v_c \\]","title":"(h)."},{"location":"assignments/a2/a2/#i","text":"\\[ \\begin{aligned} \\partial \\mathbf{J}_{\\text {skip-gram } }\\left(\\mathbf{v}_{c}, w_{t-m}, \\dots w_{t+m}, \\mathbf{U}\\right) / \\partial \\mathbf{U} &=\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, w_{t+j}, \\mathbf{U}\\right) / \\partial \\mathbf{U} \\\\ \\partial \\mathbf{J}_{\\text {skip-gram } }\\left(\\mathbf{v}_{c}, w_{t-m}, \\dots w_{t+m}, \\mathbf{U}\\right) / \\partial \\mathbf{v_c} &=\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, w_{t+j}, \\mathbf{U}\\right) / \\partial \\mathbf{v_c} \\\\ \\partial \\mathbf{J}_{\\text {skip-gram } }\\left(\\mathbf{v}_{c}, w_{t-m}, \\dots w_{t+m}, \\mathbf{U}\\right) / \\partial \\mathbf{v_w} &=\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\partial \\mathbf{J}\\left(\\mathbf{v}_{c}, w_{t+j}, \\mathbf{U}\\right) / \\partial \\mathbf{v_w} =0\\\\ \\end{aligned} \\]","title":"(i)."},{"location":"assignments/a2/a2/#2-coding-implementing-word2vec","text":"Implementation: Assignment2 Code Training Results:","title":"2 Coding: Implementing word2vec"},{"location":"assignments/a3/a3/","text":"Assignment3 Handout: CS 224n Assignment #3: Dependency Parsing 1. Machine Learning & Neural Networks (a). Adam Optimizer Adam: $$ \\begin{aligned} \\mathbf{m} & \\leftarrow \\beta_{1} \\mathbf{m}+\\left(1-\\beta_{1}\\right) \\nabla_{\\mathbf{\\theta}} J_{\\text {minibatch }}(\\mathbf{\\theta}) \\\\ \\mathbf{\\theta} & \\leftarrow \\mathbf{\\theta}-\\alpha \\mathbf{m} \\end{aligned} $$ The Adam algorithm calculates an exponential weighted moving average of the gradient which makes it perform well with nosiy or sparse gradients by minizing the variance of gradient. \\(\\mathbf{m}/{\\sqrt{v}}\\) is a normalization term which magnifies the smaller component, so the model parameters with smaller gradients get larger updates. (b). Dropout \\[ E_{p_{\\text {drop }}} \\left[\\mathbf{h}_{\\text {drop }}\\right]_{i}=E_{p_{\\text {drop }}}[\\gamma \\mathbf{d} \\odot \\mathbf{h}]_{i}=\\gamma\\left(1-p_{\\text {drop }}\\right) h_{i}\\rightarrow \\gamma=\\frac{1}{1-p_{\\text {drop }}} \\] Dropout randomly drop units from the neural network during training which prevents units from co-adapting too much. It can improve neural networks by reducing overfitting. 2. Neural Transition-Based Dependency Parsing Implementation: Assignment 3 Code (a). Complete the sequence of transitions needed for parsing the sentence \" Today I parsed a sentence \" Stack Buffer New dependency Transition [ROOT] [Today,I,parsed,a,sentence] Initial Configuration [ROOT,Today] [I,parsed,a,sentence] SHIFT [ROOT,Today,I] [parsed,a,sentence] SHIFT [ROOT,Today,I,parsed] [a,sentence] SHIFT [ROOT,Today,parsed] [a,sentence] parsed->I LEFT-ARC [ROOT,parsed] [a,sentence] parsed->Today LEFT-ARC [ROOT,parsed,a] [sentence] SHIFT [ROOT,parsed,a,sentence] [] SHIFT [ROOT,parsed,sentence] [] sentence->a LEFT-ARC [ROOT,parsed] [] parsed->sentence RIGHT-ARC [ROOT] [] ROOT->parsed RIGHT-ARC (b). A sentence containing n words will be parsed in 2n steps (c). PartialParse class: class PartialParse ( object ): def __init__ ( self , sentence ): \"\"\"Initializes this partial parse. @param sentence (list of str): The sentence to be parsed as a list of words. Your code should not modify the sentence. \"\"\" # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code. self . sentence = sentence ### YOUR CODE HERE (3 Lines) ### Your code should initialize the following fields: ### self.stack: The current stack represented as a list with the top of the stack as the ### last element of the list. ### self.buffer: The current buffer represented as a list with the first item on the ### buffer as the first item of the list ### self.dependencies: The list of dependencies produced so far. Represented as a list of ### tuples where each tuple is of the form (head, dependent). ### Order for this list doesn't matter. ### ### Note: The root token should be represented with the string \"ROOT\" ### Note: If you need to use the sentence object to initialize anything, make sure to not directly ### reference the sentence object. That is, remember to NOT modify the sentence object. self . stack = [ 'ROOT' ] self . buffer = copy . deepcopy ( sentence ) self . dependencies = [] ### END YOUR CODE def parse_step ( self , transition ): \"\"\"Performs a single parse step by applying the given transition to this partial parse @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc, and right-arc transitions. You can assume the provided transition is a legal transition. \"\"\" ### YOUR CODE HERE (~7-12 Lines) ### TODO: ### Implement a single parsing step, i.e. the logic for the following as ### described in the pdf handout: ### 1. Shift ### 2. Left Arc ### 3. Right Arc if transition == 'S' : self . stack . append ( self . buffer . pop ( 0 )) elif transition == \"LA\" : self . dependencies . append (( self . stack [ - 1 ], self . stack . pop ( - 2 ))) elif transition == 'RA' : self . dependencies . append (( self . stack [ - 2 ], self . stack . pop ( - 1 ))) ### END YOUR CODE def parse ( self , transitions ): \"\"\"Applies the provided transitions to this PartialParse @param transitions (list of str): The list of transitions in the order they should be applied @return dependencies (list of string tuples): The list of dependencies produced when parsing the sentence. Represented as a list of tuples where each tuple is of the form (head, dependent). \"\"\" for transition in transitions : self . parse_step ( transition ) return self . dependencies (d). minibatch_parse function def minibatch_parse ( sentences , model , batch_size ): \"\"\"Parses a list of sentences in minibatches using a model. @param sentences (list of list of str): A list of sentences to be parsed (each sentence is a list of words and each word is of type string) @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function model.predict(partial_parses) that takes in a list of PartialParses as input and returns a list of transitions predicted for each parse. That is, after calling transitions = model.predict(partial_parses) transitions[i] will be the next transition to apply to partial_parses[i]. @param batch_size (int): The number of PartialParses to include in each minibatch @return dependencies (list of dependency lists): A list where each element is the dependencies list for a parsed sentence. Ordering should be the same as in sentences (i.e., dependencies[i] should contain the parse for sentences[i]). \"\"\" dependencies = [] ### YOUR CODE HERE (~8-10 Lines) ### TODO: ### Implement the minibatch parse algorithm. Note that the pseudocode for this algorithm is given in the pdf handout. ### ### Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g. ### unfinished_parses = partial_parses[:]. ### Here `unfinished_parses` is a shallow copy of `partial_parses`. ### In Python, a shallow copied list like `unfinished_parses` does not contain new instances ### of the object stored in `partial_parses`. Rather both lists refer to the same objects. ### In our case, `partial_parses` contains a list of partial parses. `unfinished_parses` ### contains references to the same objects. Thus, you should NOT use the `del` operator ### to remove objects from the `unfinished_parses` list. This will free the underlying memory that ### is being accessed by `partial_parses` and may cause your code to crash. partial_parses = [ PartialParse ( sentence ) for sentence in sentences ] unfinished_parses = partial_parses [:] while unfinished_parses : transitions = model . predict ( unfinished_parses [: batch_size ]) for transition , pp in zip ( transitions , unfinished_parses [: batch_size ]): pp . parse_step ( transition ) if not len ( pp . buffer ) and len ( pp . stack ) == 1 : unfinished_parses . remove ( pp ) for partial_parse in partial_parses : dependencies . append ( partial_parse . dependencies ) ### END YOUR CODE return dependencies (e). ParseModel class class ParserModel ( nn . Module ): \"\"\" Feedforward neural network with an embedding layer and two hidden layers. The ParserModel will predict which transition should be applied to a given partial parse configuration. PyTorch Notes: - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks are a subclass of this \"nn.Module\". - The \"__init__\" method is where you define all the layers and parameters (embedding layers, linear layers, dropout layers, etc.). - \"__init__\" gets automatically called when you create a new instance of your class, e.g. when you write \"m = ParserModel()\". - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus, you should add the \"self.\" prefix layers, values, etc. that you want to utilize in other ParserModel methods. - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html. \"\"\" def __init__ ( self , embeddings , n_features = 36 , hidden_size = 200 , n_classes = 3 , dropout_prob = 0.5 ): \"\"\" Initialize the parser model. @param embeddings (ndarray): word embeddings (num_words, embedding_size) @param n_features (int): number of input features @param hidden_size (int): number of hidden units @param n_classes (int): number of output classes @param dropout_prob (float): dropout probability \"\"\" super ( ParserModel , self ) . __init__ () self . n_features = n_features self . n_classes = n_classes self . dropout_prob = dropout_prob self . embed_size = embeddings . shape [ 1 ] self . hidden_size = hidden_size self . embeddings = nn . Parameter ( torch . tensor ( embeddings )) ### YOUR CODE HERE (~9-10 Lines) ### TODO: ### 1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`. ### Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_` ### with default parameters. ### 2) Construct `self.dropout` layer. ### 3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`. ### Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_` ### with default parameters. ### ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API ### to include a tensor into a computational graph to support updating w.r.t its gradient. ### Here, we use Xavier Uniform Initialization for our Weight initialization. ### It has been shown empirically, that this provides better initial weights ### for training networks than random uniform initialization. ### For more details checkout this great blogpost: ### http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization ### ### Please see the following docs for support: ### nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters ### Initialization: https://pytorch.org/docs/stable/nn.init.html ### Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers ### ### See the PDF for hints. self . embed_to_hidden_weight = nn . Parameter ( torch . empty ( self . embed_size * n_features , self . hidden_size )) self . embed_to_hidden_bias = nn . Parameter ( torch . empty ( self . hidden_size )) self . dropout = nn . Dropout ( self . dropout_prob ) self . hidden_to_logits_weight = nn . Parameter ( torch . empty ( self . hidden_size , self . n_classes )) self . hidden_to_logits_bias = nn . Parameter ( torch . empty ( self . n_classes )) nn . init . xavier_uniform_ ( self . embed_to_hidden_weight ) nn . init . uniform_ ( self . embed_to_hidden_bias ) nn . init . xavier_uniform_ ( self . hidden_to_logits_weight ) nn . init . uniform_ ( self . hidden_to_logits_bias ) ### END YOUR CODE def embedding_lookup ( self , w ): \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings` @param w (Tensor): input tensor of word indices (batch_size, n_features) @return x (Tensor): tensor of embeddings for words represented in w (batch_size, n_features * embed_size) \"\"\" ### YOUR CODE HERE (~1-4 Lines) ### TODO: ### 1) For each index `i` in `w`, select `i`th vector from self.embeddings ### 2) Reshape the tensor using `view` function if necessary ### ### Note: All embedding vectors are stacked and stored as a matrix. The model receives ### a list of indices representing a sequence of words, then it calls this lookup ### function to map indices to sequence of embeddings. ### ### This problem aims to test your understanding of embedding lookup, ### so DO NOT use any high level API like nn.Embedding ### (we are asking you to implement that!). Pay attention to tensor shapes ### and reshape if necessary. Make sure you know each tensor's shape before you run the code! ### ### Pytorch has some useful APIs for you, and you can use either one ### in this problem (except nn.Embedding). These docs might be helpful: ### Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select ### Gather: https://pytorch.org/docs/stable/torch.html#torch.gather ### View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view ### Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html x = torch . index_select ( self . embeddings , 0 , w . flatten ()) x = x . reshape ( w . shape [ 0 ], - 1 ) ### END YOUR CODE return x def forward ( self , w ): \"\"\" Run the model forward. Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss PyTorch Notes: - Every nn.Module object (PyTorch model) has a `forward` function. - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor. For example, if you created an instance of your ParserModel and applied it to some `w` as follows, the `forward` function would called on `w` and the result would be stored in the `output` variable: model = ParserModel() output = model(w) # this calls the forward function - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward @param w (Tensor): input tensor of tokens (batch_size, n_features) @return logits (Tensor): tensor of predictions (output after applying the layers of the network) without applying softmax (batch_size, n_classes) \"\"\" ### YOUR CODE HERE (~3-5 lines) ### TODO: ### Complete the forward computation as described in write-up. In addition, include a dropout layer ### as decleared in `__init__` after ReLU function. ### ### Note: We do not apply the softmax to the logits here, because ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently. ### ### Please see the following docs for support: ### Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul ### ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu x = self . embedding_lookup ( w ) h = F . relu ( torch . matmul ( x , self . embed_to_hidden_weight ) + self . embed_to_hidden_bias ) h = self . dropout ( h ) logits = torch . matmul ( h , self . hidden_to_logits_weight ) + self . hidden_to_logits_bias ### END YOUR CODE return logits best UAS on the dev set best UAS on the test set 88.76 89.24","title":"Assignment 3"},{"location":"assignments/a3/a3/#assignment3","text":"Handout: CS 224n Assignment #3: Dependency Parsing","title":"Assignment3"},{"location":"assignments/a3/a3/#1-machine-learning-neural-networks","text":"","title":"1. Machine Learning &amp; Neural Networks"},{"location":"assignments/a3/a3/#a-adam-optimizer","text":"Adam: $$ \\begin{aligned} \\mathbf{m} & \\leftarrow \\beta_{1} \\mathbf{m}+\\left(1-\\beta_{1}\\right) \\nabla_{\\mathbf{\\theta}} J_{\\text {minibatch }}(\\mathbf{\\theta}) \\\\ \\mathbf{\\theta} & \\leftarrow \\mathbf{\\theta}-\\alpha \\mathbf{m} \\end{aligned} $$ The Adam algorithm calculates an exponential weighted moving average of the gradient which makes it perform well with nosiy or sparse gradients by minizing the variance of gradient. \\(\\mathbf{m}/{\\sqrt{v}}\\) is a normalization term which magnifies the smaller component, so the model parameters with smaller gradients get larger updates.","title":"(a). Adam Optimizer"},{"location":"assignments/a3/a3/#b-dropout","text":"\\[ E_{p_{\\text {drop }}} \\left[\\mathbf{h}_{\\text {drop }}\\right]_{i}=E_{p_{\\text {drop }}}[\\gamma \\mathbf{d} \\odot \\mathbf{h}]_{i}=\\gamma\\left(1-p_{\\text {drop }}\\right) h_{i}\\rightarrow \\gamma=\\frac{1}{1-p_{\\text {drop }}} \\] Dropout randomly drop units from the neural network during training which prevents units from co-adapting too much. It can improve neural networks by reducing overfitting.","title":"(b). Dropout"},{"location":"assignments/a3/a3/#2-neural-transition-based-dependency-parsing","text":"Implementation: Assignment 3 Code","title":"2.  Neural Transition-Based Dependency Parsing"},{"location":"assignments/a3/a3/#a-complete-the-sequence-of-transitions-needed-for-parsing-the-sentence-today-i-parsed-a-sentence","text":"Stack Buffer New dependency Transition [ROOT] [Today,I,parsed,a,sentence] Initial Configuration [ROOT,Today] [I,parsed,a,sentence] SHIFT [ROOT,Today,I] [parsed,a,sentence] SHIFT [ROOT,Today,I,parsed] [a,sentence] SHIFT [ROOT,Today,parsed] [a,sentence] parsed->I LEFT-ARC [ROOT,parsed] [a,sentence] parsed->Today LEFT-ARC [ROOT,parsed,a] [sentence] SHIFT [ROOT,parsed,a,sentence] [] SHIFT [ROOT,parsed,sentence] [] sentence->a LEFT-ARC [ROOT,parsed] [] parsed->sentence RIGHT-ARC [ROOT] [] ROOT->parsed RIGHT-ARC","title":"(a). Complete the sequence of transitions needed for parsing the sentence \"Today I parsed a sentence\""},{"location":"assignments/a3/a3/#b","text":"A sentence containing n words will be parsed in 2n steps","title":"(b)."},{"location":"assignments/a3/a3/#c","text":"PartialParse class: class PartialParse ( object ): def __init__ ( self , sentence ): \"\"\"Initializes this partial parse. @param sentence (list of str): The sentence to be parsed as a list of words. Your code should not modify the sentence. \"\"\" # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code. self . sentence = sentence ### YOUR CODE HERE (3 Lines) ### Your code should initialize the following fields: ### self.stack: The current stack represented as a list with the top of the stack as the ### last element of the list. ### self.buffer: The current buffer represented as a list with the first item on the ### buffer as the first item of the list ### self.dependencies: The list of dependencies produced so far. Represented as a list of ### tuples where each tuple is of the form (head, dependent). ### Order for this list doesn't matter. ### ### Note: The root token should be represented with the string \"ROOT\" ### Note: If you need to use the sentence object to initialize anything, make sure to not directly ### reference the sentence object. That is, remember to NOT modify the sentence object. self . stack = [ 'ROOT' ] self . buffer = copy . deepcopy ( sentence ) self . dependencies = [] ### END YOUR CODE def parse_step ( self , transition ): \"\"\"Performs a single parse step by applying the given transition to this partial parse @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc, and right-arc transitions. You can assume the provided transition is a legal transition. \"\"\" ### YOUR CODE HERE (~7-12 Lines) ### TODO: ### Implement a single parsing step, i.e. the logic for the following as ### described in the pdf handout: ### 1. Shift ### 2. Left Arc ### 3. Right Arc if transition == 'S' : self . stack . append ( self . buffer . pop ( 0 )) elif transition == \"LA\" : self . dependencies . append (( self . stack [ - 1 ], self . stack . pop ( - 2 ))) elif transition == 'RA' : self . dependencies . append (( self . stack [ - 2 ], self . stack . pop ( - 1 ))) ### END YOUR CODE def parse ( self , transitions ): \"\"\"Applies the provided transitions to this PartialParse @param transitions (list of str): The list of transitions in the order they should be applied @return dependencies (list of string tuples): The list of dependencies produced when parsing the sentence. Represented as a list of tuples where each tuple is of the form (head, dependent). \"\"\" for transition in transitions : self . parse_step ( transition ) return self . dependencies","title":"(c)."},{"location":"assignments/a3/a3/#d","text":"minibatch_parse function def minibatch_parse ( sentences , model , batch_size ): \"\"\"Parses a list of sentences in minibatches using a model. @param sentences (list of list of str): A list of sentences to be parsed (each sentence is a list of words and each word is of type string) @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function model.predict(partial_parses) that takes in a list of PartialParses as input and returns a list of transitions predicted for each parse. That is, after calling transitions = model.predict(partial_parses) transitions[i] will be the next transition to apply to partial_parses[i]. @param batch_size (int): The number of PartialParses to include in each minibatch @return dependencies (list of dependency lists): A list where each element is the dependencies list for a parsed sentence. Ordering should be the same as in sentences (i.e., dependencies[i] should contain the parse for sentences[i]). \"\"\" dependencies = [] ### YOUR CODE HERE (~8-10 Lines) ### TODO: ### Implement the minibatch parse algorithm. Note that the pseudocode for this algorithm is given in the pdf handout. ### ### Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g. ### unfinished_parses = partial_parses[:]. ### Here `unfinished_parses` is a shallow copy of `partial_parses`. ### In Python, a shallow copied list like `unfinished_parses` does not contain new instances ### of the object stored in `partial_parses`. Rather both lists refer to the same objects. ### In our case, `partial_parses` contains a list of partial parses. `unfinished_parses` ### contains references to the same objects. Thus, you should NOT use the `del` operator ### to remove objects from the `unfinished_parses` list. This will free the underlying memory that ### is being accessed by `partial_parses` and may cause your code to crash. partial_parses = [ PartialParse ( sentence ) for sentence in sentences ] unfinished_parses = partial_parses [:] while unfinished_parses : transitions = model . predict ( unfinished_parses [: batch_size ]) for transition , pp in zip ( transitions , unfinished_parses [: batch_size ]): pp . parse_step ( transition ) if not len ( pp . buffer ) and len ( pp . stack ) == 1 : unfinished_parses . remove ( pp ) for partial_parse in partial_parses : dependencies . append ( partial_parse . dependencies ) ### END YOUR CODE return dependencies","title":"(d)."},{"location":"assignments/a3/a3/#e","text":"ParseModel class class ParserModel ( nn . Module ): \"\"\" Feedforward neural network with an embedding layer and two hidden layers. The ParserModel will predict which transition should be applied to a given partial parse configuration. PyTorch Notes: - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks are a subclass of this \"nn.Module\". - The \"__init__\" method is where you define all the layers and parameters (embedding layers, linear layers, dropout layers, etc.). - \"__init__\" gets automatically called when you create a new instance of your class, e.g. when you write \"m = ParserModel()\". - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus, you should add the \"self.\" prefix layers, values, etc. that you want to utilize in other ParserModel methods. - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html. \"\"\" def __init__ ( self , embeddings , n_features = 36 , hidden_size = 200 , n_classes = 3 , dropout_prob = 0.5 ): \"\"\" Initialize the parser model. @param embeddings (ndarray): word embeddings (num_words, embedding_size) @param n_features (int): number of input features @param hidden_size (int): number of hidden units @param n_classes (int): number of output classes @param dropout_prob (float): dropout probability \"\"\" super ( ParserModel , self ) . __init__ () self . n_features = n_features self . n_classes = n_classes self . dropout_prob = dropout_prob self . embed_size = embeddings . shape [ 1 ] self . hidden_size = hidden_size self . embeddings = nn . Parameter ( torch . tensor ( embeddings )) ### YOUR CODE HERE (~9-10 Lines) ### TODO: ### 1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`. ### Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_` ### with default parameters. ### 2) Construct `self.dropout` layer. ### 3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`. ### Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_` ### with default parameters. ### ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API ### to include a tensor into a computational graph to support updating w.r.t its gradient. ### Here, we use Xavier Uniform Initialization for our Weight initialization. ### It has been shown empirically, that this provides better initial weights ### for training networks than random uniform initialization. ### For more details checkout this great blogpost: ### http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization ### ### Please see the following docs for support: ### nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters ### Initialization: https://pytorch.org/docs/stable/nn.init.html ### Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers ### ### See the PDF for hints. self . embed_to_hidden_weight = nn . Parameter ( torch . empty ( self . embed_size * n_features , self . hidden_size )) self . embed_to_hidden_bias = nn . Parameter ( torch . empty ( self . hidden_size )) self . dropout = nn . Dropout ( self . dropout_prob ) self . hidden_to_logits_weight = nn . Parameter ( torch . empty ( self . hidden_size , self . n_classes )) self . hidden_to_logits_bias = nn . Parameter ( torch . empty ( self . n_classes )) nn . init . xavier_uniform_ ( self . embed_to_hidden_weight ) nn . init . uniform_ ( self . embed_to_hidden_bias ) nn . init . xavier_uniform_ ( self . hidden_to_logits_weight ) nn . init . uniform_ ( self . hidden_to_logits_bias ) ### END YOUR CODE def embedding_lookup ( self , w ): \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings` @param w (Tensor): input tensor of word indices (batch_size, n_features) @return x (Tensor): tensor of embeddings for words represented in w (batch_size, n_features * embed_size) \"\"\" ### YOUR CODE HERE (~1-4 Lines) ### TODO: ### 1) For each index `i` in `w`, select `i`th vector from self.embeddings ### 2) Reshape the tensor using `view` function if necessary ### ### Note: All embedding vectors are stacked and stored as a matrix. The model receives ### a list of indices representing a sequence of words, then it calls this lookup ### function to map indices to sequence of embeddings. ### ### This problem aims to test your understanding of embedding lookup, ### so DO NOT use any high level API like nn.Embedding ### (we are asking you to implement that!). Pay attention to tensor shapes ### and reshape if necessary. Make sure you know each tensor's shape before you run the code! ### ### Pytorch has some useful APIs for you, and you can use either one ### in this problem (except nn.Embedding). These docs might be helpful: ### Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select ### Gather: https://pytorch.org/docs/stable/torch.html#torch.gather ### View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view ### Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html x = torch . index_select ( self . embeddings , 0 , w . flatten ()) x = x . reshape ( w . shape [ 0 ], - 1 ) ### END YOUR CODE return x def forward ( self , w ): \"\"\" Run the model forward. Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss PyTorch Notes: - Every nn.Module object (PyTorch model) has a `forward` function. - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor. For example, if you created an instance of your ParserModel and applied it to some `w` as follows, the `forward` function would called on `w` and the result would be stored in the `output` variable: model = ParserModel() output = model(w) # this calls the forward function - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward @param w (Tensor): input tensor of tokens (batch_size, n_features) @return logits (Tensor): tensor of predictions (output after applying the layers of the network) without applying softmax (batch_size, n_classes) \"\"\" ### YOUR CODE HERE (~3-5 lines) ### TODO: ### Complete the forward computation as described in write-up. In addition, include a dropout layer ### as decleared in `__init__` after ReLU function. ### ### Note: We do not apply the softmax to the logits here, because ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently. ### ### Please see the following docs for support: ### Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul ### ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu x = self . embedding_lookup ( w ) h = F . relu ( torch . matmul ( x , self . embed_to_hidden_weight ) + self . embed_to_hidden_bias ) h = self . dropout ( h ) logits = torch . matmul ( h , self . hidden_to_logits_weight ) + self . hidden_to_logits_bias ### END YOUR CODE return logits best UAS on the dev set best UAS on the test set 88.76 89.24","title":"(e)."},{"location":"assignments/a4/a4/","text":"Assignment 4 Handout: CS 224n: Assignment #4 1. Neural Machine Translation with RNNs Implementation: Assignment 4 Code In the first task, a sequence-to-sequence(Seq2Seq) network with attention is built as a Neural Translation (NMT) system for Cherokee to English translation. Training Curves: (g). generate_sent_masks() functions in nmt_model.py step() function in nmt_model.py sets \\(e_t\\) to -inf where env_masks has 1, so the attention weights will be zero fater softmax function for \"pad\" embeddings. Only the weights for real tokens in a sequence will be taken into consideration this way. # Set e_t to -inf where enc_masks has 1 if enc_masks is not None : e_t . data . masked_fill_ ( enc_masks . bool (), - float ( 'inf' )) (h). test the model sh run . sh test ( Windows ) run . bat test The result is: Decoding : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1000 / 1000 [ 00 : 56 < 00 : 00 , 17.63 it / s ] Corpus BLEU : 12.784050459913443 (i). Different Attention dot product attention: \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{h}_{i}\\) multiplicative attention: \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{W}\\mathbf{h}_{i}\\) additive attention: \\(\\mathbf{e}_{t,i} = \\mathbf{v}^{T}tanh(\\mathbf{W}_{1}\\mathbf{h}_{i}+ \\mathbf{W}_{2}\\mathbf{s}_{t})\\) 1. dot product attention compared to multiplicative attention advantage disadvantage Computing faster less expressive 2. additive attention compared to multiplicative attention advantage disadvantage More versatile heavy computing 2. Analyzing NMT Systems (a). Cherokee is a ploysynthetic language, and it will make the word vocabulary very large. Modeling at a subword-level is easier. (b). The transliterated Cherokee text might be more alignment with morphemic convention. (c). Multilingual NMT, with the inductive bias that \"the learning signal from one language should benefit the quality of translation to other languages\", is a potential remedy. (d). i. Wrong pronouns. Pronoun construction is insufficient. Adding more layers to final vocabulary projection. ii. repeated words. Putting overly high attention on adjective. Changing the attention mechanism might help. iii. Less expressive words. Low representational capacity of the model. Adding more layers to the projection layers of the encoder and decoder. (f). Computing the BLEU score References: [1]. BLEU: a Method for Automatic Evaluation of Machine Translation [2]. NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE [3]. Attention and Augmented Recurrent Neural Networks","title":"Assignment 4"},{"location":"assignments/a4/a4/#assignment-4","text":"Handout: CS 224n: Assignment #4","title":"Assignment 4"},{"location":"assignments/a4/a4/#1-neural-machine-translation-with-rnns","text":"Implementation: Assignment 4 Code In the first task, a sequence-to-sequence(Seq2Seq) network with attention is built as a Neural Translation (NMT) system for Cherokee to English translation. Training Curves:","title":"1. Neural Machine Translation with RNNs"},{"location":"assignments/a4/a4/#g-generate_sent_masks-functions-in-nmt_modelpy","text":"step() function in nmt_model.py sets \\(e_t\\) to -inf where env_masks has 1, so the attention weights will be zero fater softmax function for \"pad\" embeddings. Only the weights for real tokens in a sequence will be taken into consideration this way. # Set e_t to -inf where enc_masks has 1 if enc_masks is not None : e_t . data . masked_fill_ ( enc_masks . bool (), - float ( 'inf' ))","title":"(g). generate_sent_masks() functions in nmt_model.py"},{"location":"assignments/a4/a4/#h-test-the-model","text":"sh run . sh test ( Windows ) run . bat test The result is: Decoding : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1000 / 1000 [ 00 : 56 < 00 : 00 , 17.63 it / s ] Corpus BLEU : 12.784050459913443","title":"(h). test the model"},{"location":"assignments/a4/a4/#i-different-attention","text":"dot product attention: \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{h}_{i}\\) multiplicative attention: \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{W}\\mathbf{h}_{i}\\) additive attention: \\(\\mathbf{e}_{t,i} = \\mathbf{v}^{T}tanh(\\mathbf{W}_{1}\\mathbf{h}_{i}+ \\mathbf{W}_{2}\\mathbf{s}_{t})\\)","title":"(i). Different Attention"},{"location":"assignments/a4/a4/#1-dot-product-attention-compared-to-multiplicative-attention","text":"advantage disadvantage Computing faster less expressive","title":"1. dot product attention compared to multiplicative attention"},{"location":"assignments/a4/a4/#2-additive-attention-compared-to-multiplicative-attention","text":"advantage disadvantage More versatile heavy computing","title":"2. additive attention compared to multiplicative attention"},{"location":"assignments/a4/a4/#2-analyzing-nmt-systems","text":"(a). Cherokee is a ploysynthetic language, and it will make the word vocabulary very large. Modeling at a subword-level is easier. (b). The transliterated Cherokee text might be more alignment with morphemic convention. (c). Multilingual NMT, with the inductive bias that \"the learning signal from one language should benefit the quality of translation to other languages\", is a potential remedy. (d). i. Wrong pronouns. Pronoun construction is insufficient. Adding more layers to final vocabulary projection. ii. repeated words. Putting overly high attention on adjective. Changing the attention mechanism might help. iii. Less expressive words. Low representational capacity of the model. Adding more layers to the projection layers of the encoder and decoder. (f). Computing the BLEU score","title":"2. Analyzing NMT Systems"},{"location":"assignments/a4/a4/#references","text":"[1]. BLEU: a Method for Automatic Evaluation of Machine Translation [2]. NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE [3]. Attention and Augmented Recurrent Neural Networks","title":"References:"},{"location":"assignments/a5/a5/","text":"Assignment 5 Handout: CS 224N: Assignment 5: Self-Attention, Transformers, and Pretraining 1. Attention Exploration (a). Copying in attention i. \\(\\alpha\\) is calculating the softmax of \\(k_i^T q\\) which makes it like a categorical probability distribution. ii. To make \\(\\alpha_j \\gg \\sum_{i \\neq j} \\alpha_i\\) for some \\(j\\) . \\(k_j^T q \\gg k_i^T q, \\forall i\\neq j\\) is required. iii. under conditions give in ii. \\(c\\approx v_j\\) iv. Intuitively, the larger \\(\\alpha_i\\) is, the higher attention is given on \\(v_i\\) in the output \\(c\\) . (b). An average of two i. Let $$ M=\\left[\\begin{array}{c} a_{1}^{\\top} \\\\ \\vdots \\\\ a_{m}^{\\top} \\end{array}\\right] $$ then, \\(Ms = M(v_a+v_b)=v_a\\) ii. \\[ q = \\beta(k_a+k_b), \\beta \\gg 0 \\Longrightarrow k_a^T q= \\beta, k_b^T q = \\beta \\Longrightarrow c\\approx \\frac{1}{2}(v_a+v_b) \\] (c). Drawbacks of single-headed attention i. Since the variance is small, we can treat \\(k_i\\) as concentrating around \\(\\mu_i\\) , so $$ q=\\beta(\\mu_a+\\mu_b), \\beta\\gg 0 $$ ii. As the figures shows, \\(k_a\\) might ahve larger or smaller magnitude comparing with \\(\\mu_a\\) . To make an estimation, we can assume \\(k_a\\approx \\gamma \\mu_a, \\gamma \\sim N(1, \\frac{1}{2})\\) , so $$ c \\approx \\frac{\\exp (\\gamma \\beta)}{\\exp (\\gamma \\beta)+\\exp (\\beta)} v_{a}+\\frac{\\exp (\\beta)}{\\exp (\\gamma \\beta)+\\exp (\\beta)} v_{b} =\\frac{1}{\\exp ((1-\\gamma) \\beta)+1} v_{a}+\\frac{1}{\\exp ((\\gamma-1) \\beta)+1} v_{b} $$ i.e. \\(c\\) will oscillates between \\(v_a\\) and \\(v_b\\) as sampling \\({k_1, k_2, ..., k_n}\\) multiple times. (d). Benefits of multi-headed attention i. \\(q_1=q_2=\\beta(\\mu_a+\\mu_b), \\beta \\gg 0\\) \\(q_1 = \\mu_a, q_2 = \\mu_b\\) , or vice versa. ii. Under two circumstances in i. 1. Averaging between different samples makes \\(k_a\\) closer to its expectation \\(\\mu_a\\) , the result is that \\(c\\) is closer to \\(\\frac{1}{2}(v_a+v_b)\\) . 2. \\(c\\) is always approximately \\(\\frac{1}{2}(v_a+v_b)\\) , exactly when \\(\\alpha=0\\) . 2. Pretrained Transformer models and knowledge access Implementation: Assignment 5 Code (d). dev set accuracy all \"London\" accuracy 1.6% 5.0% (e). dev accuracy 27.6% (g). sythesize variant i. dev accuracy 23.2% ii. The single sythesizer can't capture the inter-dimension relative importance.","title":"Assignment 5"},{"location":"assignments/a5/a5/#assignment-5","text":"Handout: CS 224N: Assignment 5: Self-Attention, Transformers, and Pretraining","title":"Assignment 5"},{"location":"assignments/a5/a5/#1-attention-exploration","text":"","title":"1. Attention Exploration"},{"location":"assignments/a5/a5/#a-copying-in-attention","text":"","title":"(a). Copying in attention"},{"location":"assignments/a5/a5/#i","text":"\\(\\alpha\\) is calculating the softmax of \\(k_i^T q\\) which makes it like a categorical probability distribution.","title":"i."},{"location":"assignments/a5/a5/#ii","text":"To make \\(\\alpha_j \\gg \\sum_{i \\neq j} \\alpha_i\\) for some \\(j\\) . \\(k_j^T q \\gg k_i^T q, \\forall i\\neq j\\) is required.","title":"ii."},{"location":"assignments/a5/a5/#iii","text":"under conditions give in ii. \\(c\\approx v_j\\)","title":"iii."},{"location":"assignments/a5/a5/#iv","text":"Intuitively, the larger \\(\\alpha_i\\) is, the higher attention is given on \\(v_i\\) in the output \\(c\\) .","title":"iv."},{"location":"assignments/a5/a5/#b-an-average-of-two","text":"","title":"(b). An average of two"},{"location":"assignments/a5/a5/#i_1","text":"Let $$ M=\\left[\\begin{array}{c} a_{1}^{\\top} \\\\ \\vdots \\\\ a_{m}^{\\top} \\end{array}\\right] $$ then, \\(Ms = M(v_a+v_b)=v_a\\)","title":"i."},{"location":"assignments/a5/a5/#ii_1","text":"\\[ q = \\beta(k_a+k_b), \\beta \\gg 0 \\Longrightarrow k_a^T q= \\beta, k_b^T q = \\beta \\Longrightarrow c\\approx \\frac{1}{2}(v_a+v_b) \\]","title":"ii."},{"location":"assignments/a5/a5/#c-drawbacks-of-single-headed-attention","text":"","title":"(c). Drawbacks of single-headed attention"},{"location":"assignments/a5/a5/#i_2","text":"Since the variance is small, we can treat \\(k_i\\) as concentrating around \\(\\mu_i\\) , so $$ q=\\beta(\\mu_a+\\mu_b), \\beta\\gg 0 $$","title":"i."},{"location":"assignments/a5/a5/#ii_2","text":"As the figures shows, \\(k_a\\) might ahve larger or smaller magnitude comparing with \\(\\mu_a\\) . To make an estimation, we can assume \\(k_a\\approx \\gamma \\mu_a, \\gamma \\sim N(1, \\frac{1}{2})\\) , so $$ c \\approx \\frac{\\exp (\\gamma \\beta)}{\\exp (\\gamma \\beta)+\\exp (\\beta)} v_{a}+\\frac{\\exp (\\beta)}{\\exp (\\gamma \\beta)+\\exp (\\beta)} v_{b} =\\frac{1}{\\exp ((1-\\gamma) \\beta)+1} v_{a}+\\frac{1}{\\exp ((\\gamma-1) \\beta)+1} v_{b} $$ i.e. \\(c\\) will oscillates between \\(v_a\\) and \\(v_b\\) as sampling \\({k_1, k_2, ..., k_n}\\) multiple times.","title":"ii."},{"location":"assignments/a5/a5/#d-benefits-of-multi-headed-attention","text":"","title":"(d). Benefits of multi-headed attention"},{"location":"assignments/a5/a5/#i_3","text":"\\(q_1=q_2=\\beta(\\mu_a+\\mu_b), \\beta \\gg 0\\) \\(q_1 = \\mu_a, q_2 = \\mu_b\\) , or vice versa.","title":"i."},{"location":"assignments/a5/a5/#ii_3","text":"Under two circumstances in i. 1. Averaging between different samples makes \\(k_a\\) closer to its expectation \\(\\mu_a\\) , the result is that \\(c\\) is closer to \\(\\frac{1}{2}(v_a+v_b)\\) . 2. \\(c\\) is always approximately \\(\\frac{1}{2}(v_a+v_b)\\) , exactly when \\(\\alpha=0\\) .","title":"ii."},{"location":"assignments/a5/a5/#2-pretrained-transformer-models-and-knowledge-access","text":"Implementation: Assignment 5 Code","title":"2. Pretrained Transformer models and knowledge access"},{"location":"assignments/a5/a5/#d","text":"dev set accuracy all \"London\" accuracy 1.6% 5.0%","title":"(d)."},{"location":"assignments/a5/a5/#e","text":"dev accuracy 27.6%","title":"(e)."},{"location":"assignments/a5/a5/#g-sythesize-variant","text":"","title":"(g). sythesize variant"},{"location":"assignments/a5/a5/#i_4","text":"dev accuracy 23.2%","title":"i."},{"location":"assignments/a5/a5/#ii_4","text":"The single sythesizer can't capture the inter-dimension relative importance.","title":"ii."},{"location":"chapters/Pretraining/","text":"Pretraining 1. The Pretraining / Finetuning Paradigm Pretraining can improve NLP applicatios by serving as parameter initialization. 2. Model Pretraining Three Ways The neural architecture influences the type of pretraining, and natural use cases. Generative Pretrained Transformer (GPT) 2018's GPT was a big success in pretraining a decoder. Transfomer decoder with 12 layers. 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers. Byte-pair encoding with 40,000 merges. Trained on BooksCorpus: over 7000 unique books. BERT: Bidirectional Encoder Representations FROM Transfomers. Details about BERT Two models were released: BERT-base: 12 layers, 768-dim hidden states, 12 attention heads,110 million params. BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params. Trained on: BooksCorpus(800 million words) English Wikipedia(2,500 million words) Pretraining is expensive and impractical on a single GPU BERT was pretrained with 64 TPU chips for a total of 4 days. Finetuning is practical and common on a single GPU \"Pretrain once, finetune many times.\"","title":"Pretraining"},{"location":"chapters/Pretraining/#pretraining","text":"","title":"Pretraining"},{"location":"chapters/Pretraining/#1-the-pretraining-finetuning-paradigm","text":"Pretraining can improve NLP applicatios by serving as parameter initialization.","title":"1. The Pretraining / Finetuning Paradigm"},{"location":"chapters/Pretraining/#2-model-pretraining-three-ways","text":"The neural architecture influences the type of pretraining, and natural use cases.","title":"2. Model Pretraining Three Ways"},{"location":"chapters/Pretraining/#generative-pretrained-transformer-gpt","text":"2018's GPT was a big success in pretraining a decoder. Transfomer decoder with 12 layers. 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers. Byte-pair encoding with 40,000 merges. Trained on BooksCorpus: over 7000 unique books.","title":"Generative Pretrained Transformer (GPT)"},{"location":"chapters/Pretraining/#bert-bidirectional-encoder-representations-from-transfomers","text":"Details about BERT Two models were released: BERT-base: 12 layers, 768-dim hidden states, 12 attention heads,110 million params. BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params. Trained on: BooksCorpus(800 million words) English Wikipedia(2,500 million words) Pretraining is expensive and impractical on a single GPU BERT was pretrained with 64 TPU chips for a total of 4 days. Finetuning is practical and common on a single GPU \"Pretrain once, finetune many times.\"","title":"BERT: Bidirectional Encoder Representations FROM Transfomers."},{"location":"chapters/transformer/","text":"Transformers 1. Understanding the Transformer Model 1.1 Encoder: Self-Attention Step1: With embeddings stacked in X, calculate queries, keys and values. $$ Q=XW^Q, K=XW^K, V=XW^V $$ Step2: Calculate attention scores between query and keys $$ E=QK^T $$ Step3: Take the softmax to normalize attention scores $$ A = softmax(E) $$ Step4: Thake a weighted sum of values $$ Output = AV $$ Apply a feddforward layer to the output of attention, providing non-linear activatetion \\[ m_i = MLP(output_i) = W_2 *ReLU(W_1 \\times output_i+b_1)+b_2 \\] To make this work for deep networks: Training Trick #1: Resifual Connections Training Trick #2: LayerNorm Training Trick #3: Scaled Dot Product Attention Fixing the first self-attention problem: sequence order Solution : Inject Order Information through Positional Encodings 1.2 Multi-Headed Self-Attention 1.3 Decoder: Masked Multi-Head Self-Attention To use self-attention in decoders , we need to ensure we can't peek at the future. To enable parallelization, we mask out attention to future words by setting attention to \\(-\\infty\\) \\[ e_{i j}=\\left\\{\\begin{array}{l} q_{i}^{\\top} k_{j}, j<i \\\\ -\\infty, j \\geq i \\end{array}\\right. \\] Add a feed forward layer (with residual connections and layer norm) Add a final linear layer to project the embeddings into a much longer vector of length vocab size (logits) Add a final softmax to generate a probability distribution of possible next words. References [1]. Attention Is All You Need [2]. The Illustrated Transformer [3]. Dive into Deep Learning: Transformer [4]. CS224N: transformers [5]. Transformer\u6a21\u578b\u8be6\u89e3","title":"Transformers"},{"location":"chapters/transformer/#transformers","text":"","title":"Transformers"},{"location":"chapters/transformer/#1-understanding-the-transformer-model","text":"","title":"1. Understanding the Transformer Model"},{"location":"chapters/transformer/#11-encoder-self-attention","text":"Step1: With embeddings stacked in X, calculate queries, keys and values. $$ Q=XW^Q, K=XW^K, V=XW^V $$ Step2: Calculate attention scores between query and keys $$ E=QK^T $$ Step3: Take the softmax to normalize attention scores $$ A = softmax(E) $$ Step4: Thake a weighted sum of values $$ Output = AV $$ Apply a feddforward layer to the output of attention, providing non-linear activatetion \\[ m_i = MLP(output_i) = W_2 *ReLU(W_1 \\times output_i+b_1)+b_2 \\] To make this work for deep networks: Training Trick #1: Resifual Connections Training Trick #2: LayerNorm Training Trick #3: Scaled Dot Product Attention Fixing the first self-attention problem: sequence order Solution : Inject Order Information through Positional Encodings","title":"1.1 Encoder: Self-Attention"},{"location":"chapters/transformer/#12-multi-headed-self-attention","text":"","title":"1.2 Multi-Headed Self-Attention"},{"location":"chapters/transformer/#13-decoder-masked-multi-head-self-attention","text":"To use self-attention in decoders , we need to ensure we can't peek at the future. To enable parallelization, we mask out attention to future words by setting attention to \\(-\\infty\\) \\[ e_{i j}=\\left\\{\\begin{array}{l} q_{i}^{\\top} k_{j}, j<i \\\\ -\\infty, j \\geq i \\end{array}\\right. \\] Add a feed forward layer (with residual connections and layer norm) Add a final linear layer to project the embeddings into a much longer vector of length vocab size (logits) Add a final softmax to generate a probability distribution of possible next words.","title":"1.3 Decoder: Masked Multi-Head Self-Attention"},{"location":"chapters/transformer/#references","text":"[1]. Attention Is All You Need [2]. The Illustrated Transformer [3]. Dive into Deep Learning: Transformer [4]. CS224N: transformers [5]. Transformer\u6a21\u578b\u8be6\u89e3","title":"References"},{"location":"chapters/Embedding/GloVe/","text":"Global Vectors for Word Representation (GloVe) Original GloVe paper : GloVe: Global Vectors for Word Representation GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. Crucial Insight: Ratios of co-occurrence probabilities can encode meaning components. References [1]. Word Vectors II: GloVe, Evaluation and Training [2]. Interpreting GloVe model","title":"GloVe"},{"location":"chapters/Embedding/GloVe/#global-vectors-for-word-representation-glove","text":"Original GloVe paper : GloVe: Global Vectors for Word Representation GloVe consists of a weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful sub-structure. Crucial Insight: Ratios of co-occurrence probabilities can encode meaning components. References [1]. Word Vectors II: GloVe, Evaluation and Training [2]. Interpreting GloVe model","title":"Global Vectors for Word Representation (GloVe)"},{"location":"chapters/Embedding/Word2vec/","text":"Word2vec Introduction Original Word2vec Paper : Efficient Estimation of Word Representations in Vector Space Overview : https://myndbook.com/view/4900 Ideas: The idea is to design a model whose parameters are the word vectors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Word2vec is a software package that actually includes : 2 algorithms : continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods : negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary A detailed introduction can be found on word2vec\u4e2d\u7684\u6570\u5b66 and word2vec Parameter Learning Explained . Here only shows the derivation from the course under a simplified scenario. Objective Function For each position \\(t=1,...,T\\) , predict context words within a window of fixed size m, given center word \\(w_t\\) . Data likelihood: $$ \\text{Likelihood}=L(\\theta)=\\prod_{t=1}^{T} \\prod_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ The objective function \\(J(\\theta)\\) is the (average) negative log likelihood: $$ J(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ $$ \\text{Minimizing objective function} \\Leftrightarrow \\text{Maximizing predictive accuracy} $$ Two vectors are used for a word \\(w\\) : 1. \\(v_w\\) : when \\(w\\) is a center word. 2. \\(u_w\\) : when \\(w\\) is a context word. Then for a center word c and a context word o: $$ P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)} $$ Take derivatives to work out the minimum: $$ \\begin{aligned} \\frac{\\partial}{\\partial v_c}\\log{\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}} &= \\frac{\\partial}{\\partial v_c}\\log{\\exp \\left(u_{o}^{T} v_{c}\\right)} - \\frac{\\partial}{\\partial v_c}\\log{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\\\ &= u_o - \\sum_{x \\in V}\\frac{1}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\exp \\left(u_{x}^{T} v_{c}\\right)u_x\\\\ &= u_o - \\sum_{x \\in V}p(x|c)u_x\\\\ &= \\text{observed - expected} \\end{aligned} $$ This is just the derivatives for the center vector parameters, and the derivatives for output vector parameters are also needed (they're similar). Then we have derivative w.r.t all parameters and can minimize","title":"Word2vec"},{"location":"chapters/Embedding/Word2vec/#word2vec","text":"","title":"Word2vec"},{"location":"chapters/Embedding/Word2vec/#introduction","text":"Original Word2vec Paper : Efficient Estimation of Word Representations in Vector Space Overview : https://myndbook.com/view/4900 Ideas: The idea is to design a model whose parameters are the word vectors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Word2vec is a software package that actually includes : 2 algorithms : continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods : negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary A detailed introduction can be found on word2vec\u4e2d\u7684\u6570\u5b66 and word2vec Parameter Learning Explained . Here only shows the derivation from the course under a simplified scenario.","title":"Introduction"},{"location":"chapters/Embedding/Word2vec/#objective-function","text":"For each position \\(t=1,...,T\\) , predict context words within a window of fixed size m, given center word \\(w_t\\) . Data likelihood: $$ \\text{Likelihood}=L(\\theta)=\\prod_{t=1}^{T} \\prod_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ The objective function \\(J(\\theta)\\) is the (average) negative log likelihood: $$ J(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ $$ \\text{Minimizing objective function} \\Leftrightarrow \\text{Maximizing predictive accuracy} $$ Two vectors are used for a word \\(w\\) : 1. \\(v_w\\) : when \\(w\\) is a center word. 2. \\(u_w\\) : when \\(w\\) is a context word. Then for a center word c and a context word o: $$ P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)} $$ Take derivatives to work out the minimum: $$ \\begin{aligned} \\frac{\\partial}{\\partial v_c}\\log{\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}} &= \\frac{\\partial}{\\partial v_c}\\log{\\exp \\left(u_{o}^{T} v_{c}\\right)} - \\frac{\\partial}{\\partial v_c}\\log{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\\\ &= u_o - \\sum_{x \\in V}\\frac{1}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\exp \\left(u_{x}^{T} v_{c}\\right)u_x\\\\ &= u_o - \\sum_{x \\in V}p(x|c)u_x\\\\ &= \\text{observed - expected} \\end{aligned} $$ This is just the derivatives for the center vector parameters, and the derivatives for output vector parameters are also needed (they're similar). Then we have derivative w.r.t all parameters and can minimize","title":"Objective Function"},{"location":"chapters/LanguageModels/LSTM/","text":"Long Short-Term Memory LSTM equations Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used. - LSTM is a good default choice (especially if your dadta has particularly long dependencies, or you have lots of training data); Switch to GRUs for speed and fewer parameters. - The LSTM architecture makes it easier for the RNN to preserve information over many steps. - LSTM doesn't guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies. References [1]. http://colah.github.io/posts/2015-08-Understanding-LSTMs/ [2]. LONG SHORT-TERM MEMORY","title":"LSTM"},{"location":"chapters/LanguageModels/LSTM/#long-short-term-memory","text":"","title":"Long Short-Term Memory"},{"location":"chapters/LanguageModels/LSTM/#lstm-equations","text":"Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used. - LSTM is a good default choice (especially if your dadta has particularly long dependencies, or you have lots of training data); Switch to GRUs for speed and fewer parameters. - The LSTM architecture makes it easier for the RNN to preserve information over many steps. - LSTM doesn't guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.","title":"LSTM equations"},{"location":"chapters/LanguageModels/LSTM/#references","text":"[1]. http://colah.github.io/posts/2015-08-Understanding-LSTMs/ [2]. LONG SHORT-TERM MEMORY","title":"References"},{"location":"chapters/LanguageModels/RNN/","text":"Recurrent Neural Networks 1. RNN Language Model 2. RNN Loss and Perplexity Loss function on step \\(\\mathcal{t}\\) is cross-entropy between predicted probability distribution, and the true next word (one-hot for ): \\[ J^{(t)}(\\theta)=C E\\left(\\mathbf{y}^{(t)}, \\hat{\\mathbf{y}}^{(t)}\\right)=-\\sum_{w \\in V} \\mathbf{y}_{w}^{(t)} \\log \\hat{\\mathbf{y}}_{w}^{(t)}=-\\log \\hat{\\mathbf{y}}_{\\mathbf{x}_{t+1}}^{(t)} \\] Average this to get overall loss for entire training set: \\[ J(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J^{(t)}(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T}-\\log \\hat{\\mathbf{y}}_{\\mathbf{x}_{t+1}}^{(t)} \\] Vanishing and Exploding Gradient are serious problems, many solutions: Solution to the Exploding & Vanishing Gradients","title":"RNN"},{"location":"chapters/LanguageModels/RNN/#recurrent-neural-networks","text":"","title":"Recurrent Neural Networks"},{"location":"chapters/LanguageModels/RNN/#1-rnn-language-model","text":"","title":"1. RNN Language Model"},{"location":"chapters/LanguageModels/RNN/#2-rnn-loss-and-perplexity","text":"Loss function on step \\(\\mathcal{t}\\) is cross-entropy between predicted probability distribution, and the true next word (one-hot for ): \\[ J^{(t)}(\\theta)=C E\\left(\\mathbf{y}^{(t)}, \\hat{\\mathbf{y}}^{(t)}\\right)=-\\sum_{w \\in V} \\mathbf{y}_{w}^{(t)} \\log \\hat{\\mathbf{y}}_{w}^{(t)}=-\\log \\hat{\\mathbf{y}}_{\\mathbf{x}_{t+1}}^{(t)} \\] Average this to get overall loss for entire training set: \\[ J(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J^{(t)}(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T}-\\log \\hat{\\mathbf{y}}_{\\mathbf{x}_{t+1}}^{(t)} \\] Vanishing and Exploding Gradient are serious problems, many solutions: Solution to the Exploding & Vanishing Gradients","title":"2. RNN Loss and Perplexity"},{"location":"chapters/LanguageModels/Summary/","text":"Summary Practical Takeaways","title":"Summary"},{"location":"chapters/LanguageModels/Summary/#summary","text":"","title":"Summary"},{"location":"chapters/LanguageModels/Summary/#practical-takeaways","text":"","title":"Practical Takeaways"},{"location":"chapters/LanguageModels/languageModels/","text":"Language Models 1. Language Modeling The probability of a sequence of \\(\\mathbb{m}\\) words \\({w_1, ..., w_m}\\) is denoted as \\(P(w_1, ..., w_m)\\) , \\(P(w_1, ..., w_m)\\) is usually conditioned on a window of \\(\\mathbb{n}\\) previous words rather than all previous words: $$ P\\left(w_{1}, \\ldots, w_{m}\\right)=\\prod_{i=1}^{i=m} P\\left(w_{i} \\mid w_{1}, \\ldots, w_{i-1}\\right) \\approx \\prod_{i=1}^{i=m} P\\left(w_{i} \\mid w_{i-n}, \\ldots, w_{i-1}\\right) $$ 2. n-gram Language Models Idea: Collect statistics about how frequent different n-grams are and use these to predict next word. 2.1 Sparsity Problems 2.2 Storage Problems 3. A Fixed-window Neural LM Improvements over n-gram LM Remaining Problems No sparsity problem Fixed window is too small Don't need to store all observed n-grams No symmetry in how the inputs are processed.","title":"Language Models"},{"location":"chapters/LanguageModels/languageModels/#language-models","text":"","title":"Language Models"},{"location":"chapters/LanguageModels/languageModels/#1-language-modeling","text":"The probability of a sequence of \\(\\mathbb{m}\\) words \\({w_1, ..., w_m}\\) is denoted as \\(P(w_1, ..., w_m)\\) , \\(P(w_1, ..., w_m)\\) is usually conditioned on a window of \\(\\mathbb{n}\\) previous words rather than all previous words: $$ P\\left(w_{1}, \\ldots, w_{m}\\right)=\\prod_{i=1}^{i=m} P\\left(w_{i} \\mid w_{1}, \\ldots, w_{i-1}\\right) \\approx \\prod_{i=1}^{i=m} P\\left(w_{i} \\mid w_{i-n}, \\ldots, w_{i-1}\\right) $$","title":"1. Language Modeling"},{"location":"chapters/LanguageModels/languageModels/#2-n-gram-language-models","text":"Idea: Collect statistics about how frequent different n-grams are and use these to predict next word.","title":"2. n-gram Language Models"},{"location":"chapters/LanguageModels/languageModels/#21-sparsity-problems","text":"","title":"2.1 Sparsity Problems"},{"location":"chapters/LanguageModels/languageModels/#22-storage-problems","text":"","title":"2.2 Storage Problems"},{"location":"chapters/LanguageModels/languageModels/#3-a-fixed-window-neural-lm","text":"Improvements over n-gram LM Remaining Problems No sparsity problem Fixed window is too small Don't need to store all observed n-grams No symmetry in how the inputs are processed.","title":"3. A Fixed-window Neural LM"},{"location":"chapters/NMT/atten/","text":"Attention 1. Sequence-to-sequence: the bottleneck problem 2. Sequence-to-sequence with attention Architecture Equations Benefits Attention significantly improves NMT performance Attention provides more \"human-like\" model of the MT process Attention solves the bottleneck problem Attention helps with vanishing gradient problem Attention provides some interpretability Attention Variants There are several ways you can compute \\(\\mathbf{e}\\in \\mathbb{R}^N\\) , from \\(\\mathbf{h_1}, \\mathbf{h_2},..., \\mathbf{h_N} \\in \\mathbb{R}^{d1}\\) and \\(\\mathbf{s}\\in \\mathbb{R}^{d_2}\\) : Basic dot-product attention : \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{h}_{i}\\in \\mathbb{R}\\) Multiplicative attention : \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{W}\\mathbf{h}_{i} \\in \\mathbb{R}\\) Reduced-rank multiplicative attention : \\(e_i = s^T(\\mathbf{U^T}\\mathbf{V})h_i = (\\mathbf{U}s)^T (\\mathbf{V}h_i)\\) Additive attention : \\(\\mathbf{e}_{t,i} = \\mathbf{v}^{T}tanh(\\mathbf{W}_{1}\\mathbf{h}_{i}+ \\mathbf{W}_{2}\\mathbf{s}_{t})\\) More general definition of attention Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query.","title":"Attention"},{"location":"chapters/NMT/atten/#attention","text":"","title":"Attention"},{"location":"chapters/NMT/atten/#1-sequence-to-sequence-the-bottleneck-problem","text":"","title":"1. Sequence-to-sequence: the bottleneck problem"},{"location":"chapters/NMT/atten/#2-sequence-to-sequence-with-attention","text":"","title":"2. Sequence-to-sequence with attention"},{"location":"chapters/NMT/atten/#architecture","text":"","title":"Architecture"},{"location":"chapters/NMT/atten/#equations","text":"","title":"Equations"},{"location":"chapters/NMT/atten/#benefits","text":"Attention significantly improves NMT performance Attention provides more \"human-like\" model of the MT process Attention solves the bottleneck problem Attention helps with vanishing gradient problem Attention provides some interpretability","title":"Benefits"},{"location":"chapters/NMT/atten/#attention-variants","text":"There are several ways you can compute \\(\\mathbf{e}\\in \\mathbb{R}^N\\) , from \\(\\mathbf{h_1}, \\mathbf{h_2},..., \\mathbf{h_N} \\in \\mathbb{R}^{d1}\\) and \\(\\mathbf{s}\\in \\mathbb{R}^{d_2}\\) : Basic dot-product attention : \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{h}_{i}\\in \\mathbb{R}\\) Multiplicative attention : \\(\\mathbf{e}_{t,i} = \\mathbf{s}^{T}_{t}\\mathbf{W}\\mathbf{h}_{i} \\in \\mathbb{R}\\) Reduced-rank multiplicative attention : \\(e_i = s^T(\\mathbf{U^T}\\mathbf{V})h_i = (\\mathbf{U}s)^T (\\mathbf{V}h_i)\\) Additive attention : \\(\\mathbf{e}_{t,i} = \\mathbf{v}^{T}tanh(\\mathbf{W}_{1}\\mathbf{h}_{i}+ \\mathbf{W}_{2}\\mathbf{s}_{t})\\)","title":"Attention Variants"},{"location":"chapters/NMT/atten/#more-general-definition-of-attention","text":"Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query.","title":"More general definition of attention"},{"location":"chapters/NMT/seq2seq/","text":"NMT 1. Seq2seq Model The general notion herer is an encoder-decoder model One neural network takes input and produces a neural representation. Another network produces output based on that neural representation. If the input and output are sequences, we call it a seq2seq model. Many NLP tasks can be phrased as sequence-to-sequence: Summarization (Longtext -> short text) Dialogue (previous utterances -> next utterance) Parsing (input text -> output parse as a sequences) Code generation (natural language -> Python code) 2. Multi-layer RNNs","title":"Seq2Seq"},{"location":"chapters/NMT/seq2seq/#nmt","text":"","title":"NMT"},{"location":"chapters/NMT/seq2seq/#1-seq2seq-model","text":"The general notion herer is an encoder-decoder model One neural network takes input and produces a neural representation. Another network produces output based on that neural representation. If the input and output are sequences, we call it a seq2seq model. Many NLP tasks can be phrased as sequence-to-sequence: Summarization (Longtext -> short text) Dialogue (previous utterances -> next utterance) Parsing (input text -> output parse as a sequences) Code generation (natural language -> Python code)","title":"1. Seq2seq Model"},{"location":"chapters/NMT/seq2seq/#2-multi-layer-rnns","text":"","title":"2. Multi-layer RNNs"},{"location":"maths/SVD/","text":"SVD, reduced SVD, truncated SVD References [1] Very illuminating: https://math.stackexchange.com/a/2627161 [2] https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf","title":"SVD"},{"location":"maths/SVD/#svd-reduced-svd-truncated-svd","text":"","title":"SVD, reduced SVD, truncated SVD"},{"location":"maths/SVD/#references","text":"[1] Very illuminating: https://math.stackexchange.com/a/2627161 [2] https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf","title":"References"},{"location":"maths/derivatives/","text":"Matrix Derivative Machine learning involves plenty of matrices, vectors, and related approximations, operations, and derivatives. They are worth summarizing for subsequent reference. One important thing to notice is the Layout conventions . Wikipedia page about Matirx calculus is a good way to start. More comprehensive collection can be found on The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook : What is this? These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference . Feels like the tensor analysis in electrodynamics is back. :( References [1] Matrix Calculus [2] The Matrix Cookbook [3] \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u77e9\u9635\u3001\u5411\u91cf\u6c42\u5bfc [4] The Matrix Calculus You Need For Deep Learning [5] Mathematics for Machine Learning","title":"Matrix Derivative"},{"location":"maths/derivatives/#matrix-derivative","text":"Machine learning involves plenty of matrices, vectors, and related approximations, operations, and derivatives. They are worth summarizing for subsequent reference. One important thing to notice is the Layout conventions . Wikipedia page about Matirx calculus is a good way to start. More comprehensive collection can be found on The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook : What is this? These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference . Feels like the tensor analysis in electrodynamics is back. :(","title":"Matrix Derivative"},{"location":"maths/derivatives/#references","text":"[1] Matrix Calculus [2] The Matrix Cookbook [3] \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u77e9\u9635\u3001\u5411\u91cf\u6c42\u5bfc [4] The Matrix Calculus You Need For Deep Learning [5] Mathematics for Machine Learning","title":"References"}]}