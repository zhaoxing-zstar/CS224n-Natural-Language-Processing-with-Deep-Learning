{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CS224n Some of the important concepts, formulas, derivations, assignments, and models encountered during my self-study of CS224n: Natural Language Processing with Deep Learning will be recorded here. Layouts mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. Assignments - five assignments through the course. Assignment 1 : Introduction to word vectors References: Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft) Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. Dive into Deep Learning","title":"Home"},{"location":"#cs224n","text":"Some of the important concepts, formulas, derivations, assignments, and models encountered during my self-study of CS224n: Natural Language Processing with Deep Learning will be recorded here.","title":"CS224n"},{"location":"#layouts","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. Assignments - five assignments through the course. Assignment 1 : Introduction to word vectors","title":"Layouts"},{"location":"#references","text":"Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft) Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. Dive into Deep Learning","title":"References:"},{"location":"about/","text":"Vulgus genetrix insuetum Erat quam urbem destruitis laurea dum superstes Lorem markdownum quid: una mersis periuria vertigine et coniunctior tenetur filius Hactenus labori. Cereri adit quod ille sanguinis vellem in utrumque procumbere te perosus quales. Facit si tellus tumulum saxea flores aliter caelumque mediamque deum, Aeacideia , quae secus quae? Quid quis animalia equos altismunera solum haerentem stantemque: superbum tempora frustraque arsisses saetae in largoque inplet. Ut agant dum carpere reperta, pectore resonabilis debet dolor timendi , et igne. Capillo atras. Ipsa culpae opes nec caelestia materiamque magna Quattuor arma nunc ire mota non , a deprendere frena roboris vix maris terrarum visibus falso. Cratera et Dindymaque Cnidon, sit vitale, ecce Peneosque celeberrimus duobus. Minus est, contraque laquei piae maiora Achaide indulgens nomen monstrique latoque quod paciscor agitant trepidosque perenni centum. if (vdslIcmpFormat.northbridgeInbox(rom, copyNewlineYobibyte.google( software, wpa), 5 - horizontalDesktopPpc)) { android += captcha_ddr_denial + backlinkIsoDvd; } else { interface = touchscreen; } if (halftoneInternet(output(netiquette_it), 2 * c_uml_debug, file_secondary) <= ultra.parseViewWireless.file(process_leak_vlog, clone + rasterWebmasterBoot)) { maximize_esports.standbyHalftone = alignment_readme; twain -= bus + functionDevice(boot_sql_nosql, 12, banner_virus_dual); } vaporwareOrientation.recordServer.bootKey(5); browser_sli_token += firmware + 78 + 96; Potest pauperque venit ortu tamque, quae minimum emicat et damnatque Aiacis percutiensque. Condi fidemque dextra: tamen captus! Sibi non frutex cursu Ore nec, iamque exstinguere quam oculos, si mando. Ait illi pondere vestem conscius quos alma disponunt Typhoea, in discite verba. Armenti videbitur ignes hic, quoniam ore tantum mollis quamvis, ingemuit canum rerum Ennomon, illum. Cruore cupidine curru medio deficiunt obortis quoque invaserat, tenuisset scilicet Sibylla pedibus! Laconide quo; miseris sacra per amore erat sive petit voce, Milon dixit ipse dolore, orbis Herse Baucisque. Hanc cupit iuro mora, retinebat tamen undis. Erat pro reconditus turribus , lusisse ex dolet dictis. Diu habet hirsutaque aurum media: contemnere negant olor per corpore arma artus et cervice maesto indiciique Minervae. Vultus ille solitus nostris silva, cadunt in leto desistere vulnere in. Praelata tinctis colle vocamus mirantur refugerat oceano, summo tibi Aricinae habenas dolentius utere per concepta usa aras saecula.","title":"Vulgus genetrix insuetum"},{"location":"about/#vulgus-genetrix-insuetum","text":"","title":"Vulgus genetrix insuetum"},{"location":"about/#erat-quam-urbem-destruitis-laurea-dum-superstes","text":"Lorem markdownum quid: una mersis periuria vertigine et coniunctior tenetur filius Hactenus labori. Cereri adit quod ille sanguinis vellem in utrumque procumbere te perosus quales. Facit si tellus tumulum saxea flores aliter caelumque mediamque deum, Aeacideia , quae secus quae? Quid quis animalia equos altismunera solum haerentem stantemque: superbum tempora frustraque arsisses saetae in largoque inplet. Ut agant dum carpere reperta, pectore resonabilis debet dolor timendi , et igne. Capillo atras.","title":"Erat quam urbem destruitis laurea dum superstes"},{"location":"about/#ipsa-culpae-opes-nec-caelestia-materiamque-magna","text":"Quattuor arma nunc ire mota non , a deprendere frena roboris vix maris terrarum visibus falso. Cratera et Dindymaque Cnidon, sit vitale, ecce Peneosque celeberrimus duobus. Minus est, contraque laquei piae maiora Achaide indulgens nomen monstrique latoque quod paciscor agitant trepidosque perenni centum. if (vdslIcmpFormat.northbridgeInbox(rom, copyNewlineYobibyte.google( software, wpa), 5 - horizontalDesktopPpc)) { android += captcha_ddr_denial + backlinkIsoDvd; } else { interface = touchscreen; } if (halftoneInternet(output(netiquette_it), 2 * c_uml_debug, file_secondary) <= ultra.parseViewWireless.file(process_leak_vlog, clone + rasterWebmasterBoot)) { maximize_esports.standbyHalftone = alignment_readme; twain -= bus + functionDevice(boot_sql_nosql, 12, banner_virus_dual); } vaporwareOrientation.recordServer.bootKey(5); browser_sli_token += firmware + 78 + 96; Potest pauperque venit ortu tamque, quae minimum emicat et damnatque Aiacis percutiensque. Condi fidemque dextra: tamen captus!","title":"Ipsa culpae opes nec caelestia materiamque magna"},{"location":"about/#sibi-non-frutex-cursu","text":"Ore nec, iamque exstinguere quam oculos, si mando. Ait illi pondere vestem conscius quos alma disponunt Typhoea, in discite verba. Armenti videbitur ignes hic, quoniam ore tantum mollis quamvis, ingemuit canum rerum Ennomon, illum. Cruore cupidine curru medio deficiunt obortis quoque invaserat, tenuisset scilicet Sibylla pedibus! Laconide quo; miseris sacra per amore erat sive petit voce, Milon dixit ipse dolore, orbis Herse Baucisque. Hanc cupit iuro mora, retinebat tamen undis. Erat pro reconditus turribus , lusisse ex dolet dictis. Diu habet hirsutaque aurum media: contemnere negant olor per corpore arma artus et cervice maesto indiciique Minervae. Vultus ille solitus nostris silva, cadunt in leto desistere vulnere in. Praelata tinctis colle vocamus mirantur refugerat oceano, summo tibi Aricinae habenas dolentius utere per concepta usa aras saecula.","title":"Sibi non frutex cursu"},{"location":"styling-your-docs/","text":"","title":"Styling your docs"},{"location":"writing-your-docs/","text":"","title":"Writing your docs"},{"location":"chapters/Embedding/Word2vec/","text":"Word2vec Introduction Original Word2vec Paper : Efficient Estimation of Word Representations in Vector Space Overview : https://myndbook.com/view/4900 Ideas: The idea is to design a model whose parameters are the word vectors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Word2vec is a software package that actually includes : 2 algorithms : continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods : negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary A detailed introduction can be found on word2vec\u4e2d\u7684\u6570\u5b66 and word2vec Parameter Learning Explained . Here only shows the derivation from the course under a simplified scenario. Objective Function For each position \\(t=1,...,T\\) , predict context words within a window of fixed size m, given center word \\(w_t\\) . Data likelihood: $$ \\text{Likelihood}=L(\\theta)=\\prod_{t=1}^{T} \\prod_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ The objective function \\(J(\\theta)\\) is the (average) negative log likelihood: $$ J(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ $$ \\text{Minimizing objective function} \\Leftrightarrow \\text{Maximizing predictive accuracy} $$ Two vectors are used for a word \\(w\\) : 1. \\(v_w\\) : when \\(w\\) is a center word. 2. \\(u_w\\) : when \\(w\\) is a context word. Then for a center word c and a context word o: $$ P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)} $$ Take derivatives to work out the minimum: $$ \\begin{aligned} \\frac{\\partial}{\\partial v_c}\\log{\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}} &= \\frac{\\partial}{\\partial v_c}\\log{\\exp \\left(u_{o}^{T} v_{c}\\right)} - \\frac{\\partial}{\\partial v_c}\\log{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\\\ &= u_o - \\sum_{x \\in V}\\frac{1}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\exp \\left(u_{x}^{T} v_{c}\\right)u_x\\\\ &= u_o - \\sum_{x \\in V}p(x|c)u_x\\\\ &= \\text{observed - expected} \\end{aligned} $$ This is just the derivatives for the center vector parameters, and the derivatives for output vector parameters are also needed (they're similar). Then we have derivative w.r.t all parameters and can minimize","title":"Word2vec"},{"location":"chapters/Embedding/Word2vec/#word2vec","text":"","title":"Word2vec"},{"location":"chapters/Embedding/Word2vec/#introduction","text":"Original Word2vec Paper : Efficient Estimation of Word Representations in Vector Space Overview : https://myndbook.com/view/4900 Ideas: The idea is to design a model whose parameters are the word vectors. Then, train the model on a certain objective. At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Word2vec is a software package that actually includes : 2 algorithms : continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. 2 training methods : negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary A detailed introduction can be found on word2vec\u4e2d\u7684\u6570\u5b66 and word2vec Parameter Learning Explained . Here only shows the derivation from the course under a simplified scenario.","title":"Introduction"},{"location":"chapters/Embedding/Word2vec/#objective-function","text":"For each position \\(t=1,...,T\\) , predict context words within a window of fixed size m, given center word \\(w_t\\) . Data likelihood: $$ \\text{Likelihood}=L(\\theta)=\\prod_{t=1}^{T} \\prod_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ The objective function \\(J(\\theta)\\) is the (average) negative log likelihood: $$ J(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{m \\leq j \\leq m \\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right) $$ $$ \\text{Minimizing objective function} \\Leftrightarrow \\text{Maximizing predictive accuracy} $$ Two vectors are used for a word \\(w\\) : 1. \\(v_w\\) : when \\(w\\) is a center word. 2. \\(u_w\\) : when \\(w\\) is a context word. Then for a center word c and a context word o: $$ P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)} $$ Take derivatives to work out the minimum: $$ \\begin{aligned} \\frac{\\partial}{\\partial v_c}\\log{\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}} &= \\frac{\\partial}{\\partial v_c}\\log{\\exp \\left(u_{o}^{T} v_{c}\\right)} - \\frac{\\partial}{\\partial v_c}\\log{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\\\ &= u_o - \\sum_{x \\in V}\\frac{1}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\exp \\left(u_{x}^{T} v_{c}\\right)u_x\\\\ &= u_o - \\sum_{x \\in V}p(x|c)u_x\\\\ &= \\text{observed - expected} \\end{aligned} $$ This is just the derivatives for the center vector parameters, and the derivatives for output vector parameters are also needed (they're similar). Then we have derivative w.r.t all parameters and can minimize","title":"Objective Function"},{"location":"maths/SVD/","text":"SVD, reduced SVD, truncated SVD References [1] Very illuminating: https://math.stackexchange.com/a/2627161 [2] https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf","title":"SVD"},{"location":"maths/SVD/#svd-reduced-svd-truncated-svd","text":"","title":"SVD, reduced SVD, truncated SVD"},{"location":"maths/SVD/#references","text":"[1] Very illuminating: https://math.stackexchange.com/a/2627161 [2] https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf","title":"References"},{"location":"maths/derivatives/","text":"Matrix Derivative Machine learning involves plenty of matrices, vectors, and related approximations, operations, and derivatives. They are worth summarizing for subsequent reference. One important thing to notice is the Layout conventions . Wikipedia page about Matirx calculus is a good way to start. More comprehensive collection can be found on The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook : What is this? These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference . Feels like the tensor analysis in electrodynamics is back. :( References [1] Matrix Calculus [2] The Matrix Cookbook [3] \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u77e9\u9635\u3001\u5411\u91cf\u6c42\u5bfc","title":"Matrix Derivative"},{"location":"maths/derivatives/#matrix-derivative","text":"Machine learning involves plenty of matrices, vectors, and related approximations, operations, and derivatives. They are worth summarizing for subsequent reference. One important thing to notice is the Layout conventions . Wikipedia page about Matirx calculus is a good way to start. More comprehensive collection can be found on The Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook : What is this? These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference . Feels like the tensor analysis in electrodynamics is back. :(","title":"Matrix Derivative"},{"location":"maths/derivatives/#references","text":"[1] Matrix Calculus [2] The Matrix Cookbook [3] \u673a\u5668\u5b66\u4e60\u4e2d\u7684\u77e9\u9635\u3001\u5411\u91cf\u6c42\u5bfc","title":"References"}]}