<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <title>Pretraining &mdash; CS224n</title>
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/tonsky/FiraCode@1.206/distr/fira_code.css">
    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.8.1/css/all.css">
    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.8.1/css/v4-shims.css">
    <link rel="stylesheet" href="../../css/theme.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    <link rel="stylesheet" href="../../stylesheets/extra.css">
    <script src="//code.jquery.com/jquery-2.1.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>
        hljs.initHighlightingOnLoad();
    </script> 
</head>

<body ontouchstart="">
    <div id="container">
        <aside>
            <div class="home">
                <div class="title">
                    <button class="hamburger"></button>
                    <a href="../.." class="site-name"> CS224n</a>
                </div>
                <div class="search">
                    <div role="search">
    <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
        <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
    </form>
</div>
                </div>
            </div>
            <nav class="nav">
                <ul class="root">
                    <li class="toctree-l1"><a class="nav-item" href="../..">Home</a></li>
                    <li class="toctree-l1"><button class="section nav-item">Chapters</button>
<ul class="subnav">
    <li class="toctree-l2"><button class="section nav-item hide">Word Embeddings</button>
<ul class="subnav hide">
    <li class="toctree-l3"><a class="nav-item" href="../Embedding/Word2vec/">Word2vec</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../Embedding/GloVe/">GloVe</a></li>
</ul></li>
    <li class="toctree-l2"><button class="section nav-item hide">Language Models</button>
<ul class="subnav hide">
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/languageModels/">Language Models</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/RNN/">RNN</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/LSTM/">LSTM</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/Summary/">Summary</a></li>
</ul></li>
    <li class="toctree-l2"><button class="section nav-item hide">Neural Machine Translation</button>
<ul class="subnav hide">
    <li class="toctree-l3"><a class="nav-item" href="../NMT/seq2seq/">Seq2Seq</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../NMT/atten/">Attention</a></li>
</ul></li>
    <li class="toctree-l2"><a class="nav-item" href="../transformer/">Transformers</a></li>
    <li class="toctree-l2 current"><a class="nav-item current" href="./">Pretraining</a>
<ul class="subnav">
<li class="toctree-l3"><a class="nav-item toc" href="#1-the-pretraining-finetuning-paradigm">1. The Pretraining / Finetuning Paradigm</a></li>
<li class="toctree-l3"><a class="nav-item toc" href="#2-model-pretraining-three-ways">2. Model Pretraining Three Ways</a></li>
</ul></li>
</ul></li>
                    <li class="toctree-l1"><button class="section nav-item">Math</button>
<ul class="subnav">
    <li class="toctree-l2"><a class="nav-item" href="../../maths/derivatives/">Matrix Derivative</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../maths/SVD/">SVD</a></li>
</ul></li>
                    <li class="toctree-l1"><button class="section nav-item">Assignments</button>
<ul class="subnav">
    <li class="toctree-l2">
<a class="nav-item" href="../../assignments/a1/exploring_word_vectors.html">Assignment 1</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a2/a2/">Assignment 2</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a3/a3/">Assignment 3</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a4/a4/">Assignment 4</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a5/a5/">Assignment 5</a></li>
</ul></li>
                </ul>
            </nav>
            <div class="repo">
    <div class="previous"><a href="../transformer/">&laquo; Previous</a></div>
    <div class="next"><a href="../../maths/derivatives/">Next &raquo;</a></div>
</div>
        </aside>
        <div id="spacer"><button class="arrow"></button></div>
        <main>
            <div class="home-top">
                <button class="hamburger"></button>
                <a href="../.." class="site-name"> CS224n</a>
            </div>
            <div id="main">
                <nav class="breadcrumbs">
<ul>
    <li>Chapters</li>
</ul>
</nav>
                <div id="content"><h1 id="pretraining">Pretraining</h1>
<h2 id="1-the-pretraining-finetuning-paradigm">1. The Pretraining / Finetuning Paradigm</h2>
<p>Pretraining can improve NLP applicatios by serving as parameter initialization.</p>
<p><img alt="pretrain" src="../imgs/pretrain.png" style="height:70%;width:70%;align:center" /> </p>
<h2 id="2-model-pretraining-three-ways">2. Model Pretraining Three Ways</h2>
<p>The neural architecture influences the type of pretraining, and natural use cases.</p>
<p><img alt="threeWays" src="../imgs/threeWays.png" style="height:70%;width:70%;align:center" /> </p>
<h3 id="generative-pretrained-transformer-gpt">Generative Pretrained Transformer (GPT)</h3>
<p>2018's GPT was a big success in pretraining a decoder.</p>
<ul>
<li>Transfomer decoder with 12 layers.  </li>
<li>768-dimensional hidden states, 3072-dimensional feed-forward hidden layers.  </li>
<li>Byte-pair encoding with 40,000 merges.  </li>
<li>Trained on BooksCorpus: over 7000 unique books. </li>
</ul>
<h3 id="bert-bidirectional-encoder-representations-from-transfomers">BERT: Bidirectional Encoder Representations FROM Transfomers.</h3>
<p><img alt="bert" src="../imgs/bert.png" style="height:70%;width:70%;align:center" /> </p>
<p>Details about BERT</p>
<ul>
<li>Two models were released: <ul>
<li>BERT-base: 12 layers, 768-dim hidden states, 12 attention heads,110 million params.</li>
<li>BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params.</li>
</ul>
</li>
<li>Trained on:<ul>
<li>BooksCorpus(800 million words)</li>
<li>English Wikipedia(2,500 million words)</li>
</ul>
</li>
<li>Pretraining is expensive and impractical on a single GPU<ul>
<li>BERT was pretrained with 64 TPU chips for a total of 4 days.</li>
</ul>
</li>
<li>Finetuning is practical and common on a single GPU<ul>
<li>"Pretrain once, finetune many times."</li>
</ul>
</li>
</ul></div>
                <footer>
    <div class="footer-buttons">
        <div class="previous"><a href="../transformer/" title="Transformers"><span>Previous</span></a></div>
        <div class="next"><a href="../../maths/derivatives/" title="Matrix Derivative"><span>Next</span></a></div>
    </div>
    <div class="footer-note">
        <p>
            Built with <a href="http://www.mkdocs.org">MkDocs</a> using
            <a href="https://github.com/daizutabi/mkdocs-ivory">Ivory theme</a>.
        </p>
    </div>
</footer>
            </div>
        </main>
    </div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../javascripts/mathjax.js"></script>
    <script src="../../search/main.js"></script>
</body>

</html>