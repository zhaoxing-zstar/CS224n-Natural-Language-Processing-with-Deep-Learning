<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <title>Transformers &mdash; CS224n</title>
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/tonsky/FiraCode@1.206/distr/fira_code.css">
    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.8.1/css/all.css">
    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.8.1/css/v4-shims.css">
    <link rel="stylesheet" href="../../css/theme.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    <link rel="stylesheet" href="../../stylesheets/extra.css">
    <script src="//code.jquery.com/jquery-2.1.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>
        hljs.initHighlightingOnLoad();
    </script> 
</head>

<body ontouchstart="">
    <div id="container">
        <aside>
            <div class="home">
                <div class="title">
                    <button class="hamburger"></button>
                    <a href="../.." class="site-name"> CS224n</a>
                </div>
                <div class="search">
                    <div role="search">
    <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
        <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
    </form>
</div>
                </div>
            </div>
            <nav class="nav">
                <ul class="root">
                    <li class="toctree-l1"><a class="nav-item" href="../..">Home</a></li>
                    <li class="toctree-l1"><button class="section nav-item">Chapters</button>
<ul class="subnav">
    <li class="toctree-l2"><button class="section nav-item hide">Word Embeddings</button>
<ul class="subnav hide">
    <li class="toctree-l3"><a class="nav-item" href="../Embedding/Word2vec/">Word2vec</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../Embedding/GloVe/">GloVe</a></li>
</ul></li>
    <li class="toctree-l2"><button class="section nav-item hide">Language Models</button>
<ul class="subnav hide">
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/languageModels/">Language Models</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/RNN/">RNN</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/LSTM/">LSTM</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../LanguageModels/Summary/">Summary</a></li>
</ul></li>
    <li class="toctree-l2"><button class="section nav-item hide">Neural Machine Translation</button>
<ul class="subnav hide">
    <li class="toctree-l3"><a class="nav-item" href="../NMT/seq2seq/">Seq2Seq</a></li>
    <li class="toctree-l3"><a class="nav-item" href="../NMT/atten/">Attention</a></li>
</ul></li>
    <li class="toctree-l2 current"><a class="nav-item current" href="./">Transformers</a>
<ul class="subnav">
<li class="toctree-l3"><a class="nav-item toc" href="#1-understanding-the-transformer-model">1. Understanding the Transformer Model</a></li>
<li class="toctree-l3"><a class="nav-item toc" href="#references">References</a></li>
</ul></li>
    <li class="toctree-l2"><a class="nav-item" href="../Pretraining/">Pretraining</a></li>
</ul></li>
                    <li class="toctree-l1"><button class="section nav-item">Math</button>
<ul class="subnav">
    <li class="toctree-l2"><a class="nav-item" href="../../maths/derivatives/">Matrix Derivative</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../maths/SVD/">SVD</a></li>
</ul></li>
                    <li class="toctree-l1"><button class="section nav-item">Assignments</button>
<ul class="subnav">
    <li class="toctree-l2">
<a class="nav-item" href="../../assignments/a1/exploring_word_vectors.html">Assignment 1</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a2/a2/">Assignment 2</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a3/a3/">Assignment 3</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a4/a4/">Assignment 4</a></li>
    <li class="toctree-l2"><a class="nav-item" href="../../assignments/a5/a5/">Assignment 5</a></li>
</ul></li>
                </ul>
            </nav>
            <div class="repo">
    <div class="previous"><a href="../NMT/atten/">&laquo; Previous</a></div>
    <div class="next"><a href="../Pretraining/">Next &raquo;</a></div>
</div>
        </aside>
        <div id="spacer"><button class="arrow"></button></div>
        <main>
            <div class="home-top">
                <button class="hamburger"></button>
                <a href="../.." class="site-name"> CS224n</a>
            </div>
            <div id="main">
                <nav class="breadcrumbs">
<ul>
    <li>Chapters</li>
</ul>
</nav>
                <div id="content"><h1 id="transformers">Transformers</h1>
<p><img alt="transformer" src="../imgs/transformer.png" style="height:50%;width:50%;align:center" /></p>
<h2 id="1-understanding-the-transformer-model">1. Understanding the Transformer Model</h2>
<h3 id="11-encoder-self-attention">1.1 Encoder: Self-Attention</h3>
<ul>
<li>Step1: With embeddings stacked in X, calculate queries, keys and values.
$$
Q=XW^Q, K=XW^K, V=XW^V
$$  </li>
<li>Step2: Calculate attention scores between query and keys
$$
E=QK^T
$$  </li>
<li>Step3: Take the softmax to normalize attention scores<br />
$$
A = softmax(E)
$$  </li>
<li>Step4: Thake a weighted sum of values
$$
Output = AV
$$</li>
</ul>
<p>Apply a feddforward layer to the output of attention, providing non-linear activatetion</p>
<div class="arithmatex">\[
m_i = MLP(output_i) = W_2 *ReLU(W_1 \times output_i+b_1)+b_2
\]</div>
<p>To make this work for deep networks:<br />
Training Trick #1: Resifual Connections<br />
Training Trick #2: LayerNorm<br />
Training Trick #3: Scaled Dot Product Attention<br />
<img alt="scaled_dot_product" src="../imgs/scaleDot.png" style="height:50%;width:50%;align:center" />   </p>
<hr />
<p>Fixing the first self-attention problem: <strong>sequence order</strong></p>
<p><strong>Solution</strong>: Inject Order Information through Positional Encodings</p>
<hr />
<h3 id="12-multi-headed-self-attention">1.2 Multi-Headed Self-Attention</h3>
<p><img alt="multi_headed_attention" src="../imgs/multiheadAtt.png" style="height:50%;width:50%;align:center" /> </p>
<h3 id="13-decoder-masked-multi-head-self-attention">1.3 Decoder: Masked Multi-Head Self-Attention</h3>
<ul>
<li>To use self-attention in <strong>decoders</strong>, we need to ensure we can't peek at the future. To enable parallelization, we mask out attention to future words by setting attention to <span class="arithmatex">\(-\infty\)</span>  </li>
</ul>
<div class="arithmatex">\[
e_{i j}=\left\{\begin{array}{l}
q_{i}^{\top} k_{j}, j&lt;i \\
-\infty, j \geq i
\end{array}\right.
\]</div>
<ul>
<li>Add a feed forward layer (with residual connections and layer norm)  </li>
<li>Add a final linear layer to project the embeddings into a much longer vector of length vocab size (logits)  </li>
<li>Add a final softmax to generate a probability distribution of possible next words.</li>
</ul>
<h2 id="references">References</h2>
<p>[1]. <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a><br />
[2]. <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a><br />
[3]. <a href="https://d2l.ai/chapter_attention-mechanisms/transformer.html?highlight=transformer">Dive into Deep Learning: Transformer</a><br />
[4]. <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture09-transformers.pdf">CS224N: transformers</a><br />
[5]. <a href="https://www.jianshu.com/p/9b87b945151e">Transformer模型详解</a></p></div>
                <footer>
    <div class="footer-buttons">
        <div class="previous"><a href="../NMT/atten/" title="Attention"><span>Previous</span></a></div>
        <div class="next"><a href="../Pretraining/" title="Pretraining"><span>Next</span></a></div>
    </div>
    <div class="footer-note">
        <p>
            Built with <a href="http://www.mkdocs.org">MkDocs</a> using
            <a href="https://github.com/daizutabi/mkdocs-ivory">Ivory theme</a>.
        </p>
    </div>
</footer>
            </div>
        </main>
    </div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../javascripts/mathjax.js"></script>
    <script src="../../search/main.js"></script>
</body>

</html>